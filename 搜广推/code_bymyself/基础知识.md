# 基础知识
## Logistic回归

+ sigmoid公式：
$$
f(x)=\frac{1}{1+e^{-x}}
$$

+ sigmoid函数图像

<img src="http://ryluo.oss-cn-chengdu.aliyuncs.com/图片Sigmoid_function.png" alt="Sigmoid function" style="display: block; margin: 0 auto;">

+ 导数
\[
\sigma'(x) = \sigma(x) \cdot (1 - \sigma(x))
\]

+ LR分类器 ==二分类==
+ 表达式
$$
p_i(y_i=1 \mid x_i) = \frac{1}{1+e^{-w x_i}} \\ \\
p_i(y_i=0 \mid x_i) = \frac{e^{-w x_i}}{1+e^{-w x_i}}
$$
    其中，随机变量$x_i \in \mathbb{R}^{n}$为实数，随机变量$y_i$的取值为$\{0, 1\}$，参数$w\in \mathbb{R}^{n}$。$wx_i$表示变量$x_i$与参数$w$之间的内积。

+ 几率 ： ==事件发生概率与不发生的概率的比值
$$
\eta_{i}=\frac{p_{i}}{1-p_{i}}
$$

    将等式两边化为以`e`为底的指数函数，有：
$$
e^{wx_i} =\frac{p_{i}}{1-p_{i}}
\\ \Downarrow
\\
p_i = \frac{1}{1+e^{-w x_i}}
$$
    逻辑回归本质上就是**关于事件几率的线性回归**。当然，逻辑回归和线性回归存在本质上的不同，其次在损失函数上，线性回归中的损失函数是均方误差，而 Logistic 回归的损失函数是**负对数似然（Negative Log-Likelihood）**。

+ 假设

    逻辑回归模型的基本假设是$y_i$服从伯努利分布，也称为两点分布或者$0-1$分布。即：
$$
p_i(y_i=1 \mid x_i ; w)=p_i \\
p_i(y_i=0 \mid x_i ; w)=1-p_i
$$

    将公式写在一起有：

$$
p_i(y_i \mid x_i ; w)=p_i^{y_i}\left(1-p_i\right)^{1-y_i}
$$


+ 参数估计

    设似然函数为：
$$
\prod_{i=1}^{n}p_{i}^{y_{i}}·\left(1-p_{i}\right)^{1-y_{i}}
$$
    对似然函数取负对数有：
$$
L(w)=-\sum_{i=1}^{n}\left(y_{i} \log \left(p_{i}\right)+\left(1-y_{i}\right) \log \left(1-p_{i}\right)\right)
$$
    通过最大化似然函数来对参数进行估计，在这里等价于最小化负对数似然函数 $L(w)$，同样可以得到 $w$ 的估计值。

    下面关于 $L(w)$ 对 $w$ 求导，具体步骤如下：

1. 对于任意样本$x_i$，有：

$$
\begin{aligned}
l &=-y_{i} \log \left(p_{i}\right)-\left(1-y_{i}\right) \log \left(1-p_{i}\right) \\ \\
&=-y_{i} \log \left(p_{i}\right)-\log \left(1-p_{i}\right)+y_{i} \log \left(1-p_{i}\right) \\ \\
&=-y_{i}\left(\log \left(\frac{p_{i}}{1-p_{i}}\right)\right)-\log \left(1-p_{i}\right)
\end{aligned}
$$
2. 由于几率 $\eta_{i}=\frac{p_{i}}{1-p_{i}}$，可得 ${p_i}=\frac{\eta_{i}}{1+\eta_{i}}$，代入：

$$
\begin{aligned}
l &=-y_{i} \log \left(\eta_{i}\right)-\log \left(1-\frac{\eta_{i}}{1+\eta_{i}}\right) \\ \\
&=-y_{i} \log \left(\eta_{i}\right)+\log \left({1+\eta_{i}}\right) \\ \\
&=-y_{i} \log \left(\eta_{i}\right)+\log \left(1+e^{\log \left(\eta_{i}\right)}\right)
\end{aligned}
$$
3. 对几率的对数 $\log(\eta_{i})$ 求导：

$$
\frac{d l}{d \log \left(\eta_{i}\right)}=-y_{i}+\frac{e^{\log \left(\eta_{i}\right)}}{1+e^{\log \left(\eta_{i}\right)}}=-y_{i}+p_{i}
$$

提到过，逻辑回归相当于**对事件的对数几率拟合线性回归**，即：$\log \left(\eta_{i}\right)=\log \frac{p_{i}}{1-p_{i}}=wx_i$，代入有：

$$
\frac{d l}{d \log \left(\eta_{i}\right)} 
=\frac{d l}{d (wx_i)}==-y_{i}+p_{i}
\\
\Downarrow
\\
\frac{d l}{dw}=(-y_{i}+p_{i})x_i
$$
    由于目标是最小化负对数似然函数，所以沿着梯度下降方向：
$$
w \leftarrow w-\frac{\gamma}{n} \sum_{i=1}^{n}\left(-y_{i}+p_{i}\right) x_{i}
\\
其中，\gamma为学习率
$$

## 相似性度量方法
1. **杰卡德（Jaccard）相似系数**
   
   `Jaccard` 系数是衡量**两个集合**的相似度一种指标，计算公式如下：
   $$
   sim_{uv}=\frac{|N(u) \cap N(v)|}{|N(u) \cup N(v)|}
   $$

   + 其中 $N(u)$，$N(v)$ 分别表示用户 $u$ 和用户 $v$ 交互物品的集合。
   
   + 对于用户 $u$ 和 $v$ ，该公式反映了两个交互物品交集的数量占这两个用户交互物品并集的数量的比例。
   
   杰卡德相似系数常用来评估**用户是否会对某物品进行打分**。
   
2. **余弦相似度**
   余弦相似度衡量了两个向量的夹角，夹角越小越相似。余弦相似度的计算如下，其与杰卡德（Jaccard）相似系数只是在分母上存在差异：
   $$
   sim_{uv}=\frac{|N(u) \cap N(v)|}{\sqrt{|N(u)|\cdot|N(v)|}}
   $$
   从向量的角度来看，令矩阵 $A$ 为用户-物品交互矩阵，矩阵的行表示用户，列表示物品。
   
   + 设用户和物品数量分别为 $m,n$，交互矩阵$A$就是一个 $m$ 行 $n$ 列的矩阵。
   
   + 矩阵中的元素均为 $0/1$。若用户 $i$ 对物品 $j$ 存在交互，那么 $A_{i,j}=1$，否则为 $0$ 。

   + 那么，用户之间的相似度为：
     $$
     sim_{uv} = cos(u,v) =\frac{u\cdot v}{|u|\cdot |v|}
     $$
   
     + 向量 $u,v$ 在形式都是 one-hot 类型，$u\cdot v$ 表示向量点积。
   
   上述用户-物品交互矩阵在现实中是十分稀疏的，为了节省内存，交互矩阵会采用**字典**进行存储。在 `sklearn` 中，余弦相似度的实现：

3. **皮尔逊相关系数**

   在用户之间的余弦相似度计算时，**将用户向量的内积展开为各元素乘积和**：
   $$
   sim_{uv} = \frac{\sum_i r_{ui}*r_{vi}}{\sqrt{\sum_i r_{ui}^2}\sqrt{\sum_i r_{vi}^2}}
   $$
   + 其中，$r_{ui},r_{vi}$ 分别表示用户 $u$ 和用户 $v$ 对物品 $i$ 是否有交互(或具体评分值)。
   
   皮尔逊相关系数计算公式：
   $$
   sim(u,v)=\frac{\sum_{i\in I}(r_{ui}-\bar r_u)(r_{vi}-\bar r_v)}{\sqrt{\sum_{i\in I }(r_{ui}-\bar r_u)^2}\sqrt{\sum_{i\in I }(r_{vi}-\bar r_v)^2}}
   $$
   + 其中，$r_{ui},r_{vi}$ 分别表示用户 $u$ 和用户 $v$ 对物品 $i$ 是否有交互(或具体评分值)；
   + $\bar r_u, \bar r_v$ 分别表示用户 $u$ 和用户 $v$ 交互的所有物品交互数量或者评分的平均值；
   
   相较于余弦相似度，皮尔逊相关系数通过使用**用户的平均分**对各独立评分进行修正，减小了用户评分偏置的影响。
   

4. **适用场景**

+ $Jaccard$ 相似度表示两个集合的交集元素个数在并集中所占的比例 ，所以适用于隐式反馈数据（0-1）。
+ 余弦相似度在度量文本相似度、用户相似度、物品相似度的时候都较为常用。
+ 皮尔逊相关度，**实际上也是一种余弦相似度。不过先对向量做了中心化**，范围在 $-1$ 到 $1$。
  + 相关度量的是两个变量的变化趋势是否一致，两个随机变量是不是同增同减。
  + 不适合用作计算布尔值向量（0-1）之间相关度。

## 评价指标

1. 混淆矩阵
   
| **预测结果**     | **实际为正**       | **实际为负**       |
|------------------|--------------------|--------------------|
| **预测为正**     | True Positive (TP)  | False Positive (FP) |
| **预测为负**     | False Negative (FN) | True Negative (TN)  |

2. 召回率

    对用户 $u$ 推荐 $N$ 个物品记为 $R(u)$, 令用户 $u$ 在测试集上喜欢的物品集合为$T(u)$， 那么召回率定义为：

$$ \text{Recall} = \frac{\text{TP}}{\text{TP} + \text{FN}} $$

+ 含义：在模型召回预测的物品中，预测准确的物品占用户实际喜欢的物品的比例。 

3. 精确率
精确率定义为：
$$ \text{Precision} = \frac{\text{TP}}{\text{TP} + \text{FP}} $$ 
+ 含义：推荐的物品中，对用户准确推荐的物品占总物品的比例。 
+ 如要确保召回率高，一般是推荐更多的物品，期望推荐的物品中会涵盖用户喜爱的物品。而实际中，推荐的物品中用户实际喜爱的物品占少数，推荐的精确率就会很低。故同时要确保高召回率和精确率往往是矛盾的，所以实际中需要在二者之间进行权衡。

4. ROC曲线与AUC
   ROC曲线通过计算真正例率(TPR)和假正例率(FPR)值，然后绘制 FPR的TPR图表。ROC 曲线的左上角表示理想模型（TPR接近1，FPR 接近 0）。完全随机的模型对应于对角线（对角线上的任何点表示在某个阈值下随机猜测的模型效果。曲线越靠近左上角，说明模型性能越好。

   * True Positive Rate (TPR)
TPR（recall）表示在所有实际为正的样本中，模型正确预测为正的比例，公式为：

\[
\text{TPR} = \frac{\text{TP}}{\text{TP} + \text{FN}}
\]

   * False Positive Rate (FPR)
FPR 表示在所有实际为负的样本中，模型错误预测为正的比例，公式为：

\[
\text{FPR} = \frac{\text{FP}}{\text{FP} + \text{TN}}
\]

AUC(曲线下面积)，0.5是一个模型预测能力的分界点。小于0.5就是在胡乱猜测。
| AUC = 0.65 | AUC = 0.93 |
|------------|------------|
| ![AUC 0.65](https://developers.google.com/static/machine-learning/crash-course/images/auc_0-65.png?hl=zh-cn) | ![AUC 0.93](https://developers.google.com/static/machine-learning/crash-course/images/auc_0-93.png?hl=zh-cn) |


5. 覆盖率
    覆盖率反映了**推荐算法发掘长尾**的能力，覆盖率越高，说明推荐算法越能将长尾中的物品推荐给用户。
$$
\text { Coverage }=\frac{\left|\bigcup_{u \in U} R(u)\right|}{|I|}
$$

+ 含义：推荐系统能够推荐出来的物品占总物品集合的比例。
  + 其中 $|I|$ 表示所有物品的个数；
  + 系统的用户集合为$U$;
  + 推荐系统给每个用户推荐一个长度为 $N$ 的物品列表$R(u)$.

+ 覆盖率表示最终的推荐列表中包含多大比例的物品。如果所有物品都被给推荐给至少一个用户， 那么覆盖率是100%。