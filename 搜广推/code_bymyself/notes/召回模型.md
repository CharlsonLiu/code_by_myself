# 基于协同过滤的召回
## Use-CF & Item-CF

**示例图片**
![image-20210629232622758](http://ryluo.oss-cn-chengdu.aliyuncs.com/图片image-20210629232622758.png)

### user

**计算过程**
1. 计算用户之间的相似度

   + 根据几种评价指标,计算出各用户之间的相似程度。对于用户 Alice，选取出与其最相近的 $N$ 个用户。

2. 计算用户对新物品的评分预测

   + 方法一：利用目标用户与相似用户之间的相似度以及相似用户对物品的评分，来预测目标用户对候选物品的评分估计：
     $$
     R_{\mathrm{u}, \mathrm{p}}=\frac{\sum_{\mathrm{s} \in S}\left(w_{\mathrm{u}, \mathrm{s}} \cdot R_{\mathrm{s}, \mathrm{p}}\right)}{\sum_{\mathrm{s} \in S} w_{\mathrm{u}, \mathrm{s}}}
     $$

     + 其中，权重 $w_{u,s}$ 是用户 $u$ 和用户 $s$ 的相似度， $R_{s,p}$ 是用户 $s$ 对物品 $p$ 的评分。

   + 方法二：考虑到用户评分的偏置，即有的用户喜欢打高分， 有的用户喜欢打低分的情况。公式如下：
     $$
     R_{\mathrm{u}, \mathrm{p}}=\bar{R}_{u} + \frac{\sum_{\mathrm{s} \in S}\left(w_{\mathrm{u}, \mathrm{s}} \cdot \left(R_{s, p}-\bar{R}_{s}\right)\right)}{\sum_{\mathrm{s} \in S} w_{\mathrm{u}, \mathrm{s}}}
     $$

     + 其中，$\bar{R}_{s}$ 表示用户 $s$ 对物品的历史平均评分。

3. 对用户进行物品推荐

   + 在获得用户 $u$ 对不同物品的评价预测后， 最终的推荐列表根据预测评分进行排序得到。 

### item

计算过程
1. 计算物品之间的相似度：余弦相似性或者皮尔逊相关系数。选取TopN相似物品计算评分
2. 根据选取出来的TopN，与user的计算方式相同，计算得分。

由于物品推荐存在长尾效应，需要对推荐权重进行控制，以防止推荐内容过于同质化。

* base 公式
  $$
  w_{i j}=\frac{|N(i) \bigcap N(j)|}{|N(i)|}
  $$

  + 该公式表示同时喜好物品 $i$ 和物品 $j$ 的用户数，占喜爱物品 $i$ 的比例。
  + 缺点：若物品 $j$ 为热门物品，那么它与任何物品的相似度都很高。

* 控制对热门物品的惩罚力度
  $$
  w_{i j}=\frac{|N(i) \cap N(j)|}{|N(i)|^{1-\alpha}|N(j)|^{\alpha}}
  $$

  * 参数 $\alpha$为可控惩罚力度。

* 对活跃用户的惩罚

  * 在计算物品之间的相似度时，可以进一步将用户的活跃度考虑进来。
    $$
    w_{i j}=\frac{\sum_{\operatorname{\text {u}\in N(i) \cap N(j)}} \frac{1}{\log 1+|N(u)|}}{|N(i)|^{1-\alpha}|N(j)|^{\alpha}}
    $$

### 弊端
1. 泛化能力弱，无法将两个物品的相似信息推广到其他物品上，进而导致热门物品具有**很强的头部效应**，容易跟大量物品产生相似，而尾部物品由于特征向量稀疏，导致很少被推荐，也就是推荐时的同质化。
2. 需要考虑用户评分倾向，一部分用户倾向高分，一部分用户倾向低分。需要对相似度计算进行改进，否则会造成错误分类
3. 没有利用更多的用户或者物品信息，只使用交互信息进行计算，整体模型的表达能力十分有限。

## 矩阵分解

+ 原理图：
<img src="https://img-blog.csdnimg.cn/20200822212051499.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3d1emhvbmdxaWFuZw==,size_1,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述" style="zoom:80%;" />

+ 基本思想
  - 用户矩阵Q
    表示不同用户**对于不同元素的偏好程度**，取值范围[0,1]，1代表很喜欢，0代表不喜欢。
  - 物品矩阵P
    表示**每种物品含有各种元素（特征）的成分**
  - 计算：
    通过将对应向量的元素做内积运算，得到用户对该物品的喜好程度。相应的可以得到用户对每个物品的评分矩阵

+ FunkSVD with Bias 算法
  - 思想：**把求解P、Q的参数问题转换成最优化问题，通过训练集里面的观察值,利用最小化来学习用户矩阵和物品矩阵。**
  - 算法过程
  1. 在有用户矩阵和物品矩阵的前提下，若要计算用户 $u$ 对物品 $i$ 的评分， 可以根据公式：
   $$
   \operatorname{Preference}(u, i)=r_{u i}=p_{u}^{T} q_{i}=\sum_{k=1}^{K} p_{u, k} q_{i,k}
   $$
   其中，向量 $p_u$ 表示用户 $u$ 的隐向量，向量  $q_i$ 表示物品 $i$ 的隐向量。

  2. **随机初始化**一个用户矩阵 $U$ 和一个物品矩阵 $V$，获取每个用户和物品的初始隐语义向量。一般是与`1/sqrt(F)`成正比，F为隐向量维度。

  3. 计算预测得分 with bias
   $$\hat{r}_{u i}=\mu+b_{u}+b_{i}+p_{u}^{T} \cdot q_{i}$$.其中，$\mu$反映推荐模型整体的平均评分，一般为样本均值；$b_u$为用户偏差系数。可以使用用户 $u$ 的评分均值，也可作为训练参数；$b_i$为物品偏差系数。可以使用物品 $i$ 的得分均值，也可以当做训练参数。

  4. 对于评分矩阵中的每个元素，计算预测误差 $e_{u i}=r_{u i}-\hat{r}_{u i}$，对所有训练样本的平方误差进行累加：
   $$
   \operatorname{SSE}=\sum_{u, i} e_{u i}^{2}=\sum_{u, i}\left(r_{u i}-\sum_{k=1}^{K} p_{u,k} q_{i,k}\right)^{2}
   $$为方便后续求解，给 $SSE$ 增加系数 $1/2$ ：
     $$
     \operatorname{SSE}=\frac{1}{2} \sum_{u, i} e_{u i}^{2}=\frac{1}{2} \sum_{u, i}\left(r_{u i}-\sum_{k=1}^{K} p_{u k} q_{i k}\right)^{2}
     $$

  5.  前面提到，模型预测越准确等价于预测误差越小，那么优化的目标函数变为：
  $$
  \begin{aligned}
  \min _{q^{*}, p^{*}} \frac{1}{2} \sum_{(u, i) \in K} &\left(r_{u i}-\left(\mu+b_{u}+b_{i}+q_{i}^{T} p_{u}\right)\right)^{2} \\
  &+\lambda\left(\left\|p_{u}\right\|^{2}+\left\|q_{i}\right\|^{2}+b_{u}^{2}+b_{i}^{2}\right)
  \end{aligned}
  $$ 
  $K$ 表示所有用户评分样本的集合，**即评分矩阵中不为空的元素**，其他空缺值在测试时是要预测的。该目标函数需要优化的目标是用户矩阵 $U$ 和一个物品矩阵 $V$。

  1. 对于给定的目标函数，可以通过梯度下降法对参数进行优化。

   + 求解目标函数 $SSE$ 关于用户矩阵中参数 $p_{u,k}$ 的梯度：
     $$
     \frac{\partial}{\partial p_{u,k}} S S E=\frac{\partial}{\partial p_{u,k}}\left(\frac{1}{2}e_{u i}^{2}\right) =e_{u i} \frac{\partial}{\partial p_{u,k}} e_{u i}=e_{u i} \frac{\partial}{\partial p_{u,k}}\left(r_{u i}-\sum_{k=1}^{K} p_{u,k} q_{i,k}\right)=-e_{u i} q_{i,k}
     $$

   + 求解目标函数 $SSE$ 关于 $q_{i,k}$ 的梯度：
     $$
     \frac{\partial}{\partial q_{i,k}} S S E=\frac{\partial}{\partial q_{i,k}}\left(\frac{1}{2}e_{u i}^{2}\right) =e_{u i} \frac{\partial}{\partial q_{i,k}} e_{u i}=e_{u i} \frac{\partial}{\partial q_{i,k}}\left(r_{u i}-\sum_{k=1}^{K} p_{u,k} q_{i,k}\right)=-e_{u i} p_{u,k}
     $$

  2. 参数梯度更新
   $$
   p_{u, k}=p_{u,k}-\eta (-e_{ui}q_{i, k})=p_{u,k}+\eta e_{ui}q_{i, k} \\ 
   q_{i, k}=q_{i,k}-\eta (-e_{ui}p_{u,k})=q_{i, k}+\eta e_{ui}p_{u, k}\\
   \frac{\partial}{\partial b_{i}} S S E=-e_{u i}+\lambda b_{i}\\
   \frac{\partial}{\partial b_{u}} S S E=-e_{u i} +\lambda b_{u}
   $$ 其中，$\eta$ 表示学习率，用于控制步长。当**参数很多**的时候，就是两个矩阵很大的时候，往往容易陷入**过拟合**的困境，需要在目标函数上面加上正则化的损失。

  - 原理图，==分解出的k维特征是模型待学习的参数==
<div align=center>
<img src="https://img-blog.csdnimg.cn/20200823101513233.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3d1emhvbmdxaWFuZw==,size_1,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述" style="zoom:70%;" />
</div>

## Neural Collaborative Filtering (NeuralCF)

### 模型结构
<div align="center">
  <img src="https://static001.geekbang.org/resource/image/5f/2c/5ff301f11e686eedbacd69dee184312c.jpg" alt="NeuralCF Model" style="zoom:33%;">
</div>

### 原理
- **输入特征**：用户ID和项目ID转换为OneHot向量，并映射成指定维度的稠密向量，解决冷启动问题。
- **GMF（广义矩阵分解部分）**：用户与物品特征矩阵逐元素点积，通过线性方法学习特征。
- **MLP**：设置用户和物品的MLP，通过特征组合拼接embedding，经过线性层得到MLP的输出。
- **最终输出**：拼接GMF与MLP特征，通过线性层得到最终输出。

### 优化
$$
L_{sqr}=\sum_{(u,i)\in y\cup y^-}w_{ui}(y_{ui}-\hat{y}_{ui})^2
$$
其中 $w_{ui}$ 为超参数，给每个样本赋权重。

$$
p(y,y^-|P,Q,\Theta_f)=\prod_{(u,i)\in{y}}\hat{y}_{ui}\prod_{(u,j)\in{y^-}}(1-\hat{y}_{uj})
$$

$$
L=-\sum_{(u,i)\in{y}}log\hat{y}_{ui}-\sum_{(u,j)\in{y^-}}log(1-\hat{y}_{uj})=-\sum_{(u,i)\in{y}\cup{y}^-}y_{ui}log \hat{y}_{ui}+(1-y_{ui})log(1-\hat{y}_{ui})
$$
使用随机梯度下降（SGD）进行训练优化。这个函数等价于交叉熵损失函数(binary cross-entropy loss)。通过对NCF进行概率处理，将隐性反馈的推荐问题当做二分类问题来解决。对于负样本 $y^-$，每次迭代均匀地从未观察到交互作用中采样作为负样本，控制采样比例。

**注**：MF是NCF模型的一个特例。

# 基于向量召回
## FM模型（分解机）

### 模型形式

$$
y = w_0+\sum_{i=1}^nw_ix_i+\sum_{i=1}^{n}\sum_{i+1}^n\lt v_i,v_j\gt x_ix_j
$$

改进思想：**为每个 $x_i$ 计算一个 embedding，然后将两个向量之间的 embedding 做内积得到之前所谓的 $w_{ij}$**。这使得即使两个特征之前从未在训练集中**同时**出现，只需要 $x_i$ 和其他的 $x_k$ 同时出现过就可以计算出 $x_i$ 的 embedding，大大提升了模型的泛化能力。**当交叉项参数 $w_{ij}$ 全为 0 的时候，整个模型就退化为普通的 LR 模型**。

---

公式中各参数的意义：

- $\omega_{0}$：**全局偏置**；
- $\omega_{i}$：模型第 $i$ 个变量的权重；
- $\omega_{ij} = < v_{i}, v_{j}>$：特征 $i$ 和 $j$ 的交叉权重；
- $v_{i}$：第 $i$ 维特征的隐向量；
- $<\cdot, \cdot>$：向量点积；
- $k(k<<n)$：隐向量的长度，包含 $k$ 个描述特征的因子。

---

* 降低时间复杂度的证明

利用 $\sum$ 运算的线性性质，公式可以化简为：

$$
\begin{align} 
\sum_{i=1}^{n-1}{\sum_{j=i+1}^{n}{<v_i,v_j>x_ix_j}}
&= \frac{1}{2}\sum_{i=1}^{n}{\sum_{j=1}^{n}{<v_i,v_j>x_ix_j}} - \frac{1}{2} {\sum_{i=1}^{n}{<v_i,v_i>x_ix_i}} \\
&= \frac{1}{2} \left( \sum_{i=1}^{n}{\sum_{j=1}^{n}{\sum_{f=1}^{k}{v_{i,f}v_{j,f}x_ix_j}}} - \sum_{i=1}^{n}{\sum_{f=1}^{k}{v_{i,f}v_{i,f}x_ix_i}} \right) \\
&= \frac{1}{2}\sum_{f=1}^{k}{\left[ \left( \sum_{i=1}^{n}{v_{i,f}x_i} \right) \cdot \left( \sum_{j=1}^{n}{v_{j,f}x_j} \right) - \sum_{i=1}^{n}{v_{i,f}^2 x_i^2} \right]} \\
&= \frac{1}{2}\sum_{f=1}^{k}{\left[ \left( \sum_{i=1}^{n}{v_{i,f}x_i} \right)^2 - \sum_{i=1}^{n}{v_{i,f}^2 x_i^2} \right]} 
\end{align}
$$

---

* 解释

- $v_{i,f}$ 是一个具体的值；
- **第 1 个等号**：对称矩阵 $W$ 的对角线上半部分；
- **第 2 个等号**：将向量内积 $<v_i, v_j>$ 展开为累加和的形式；
- **第 3 个等号**：提出公共部分；
- **第 4 个等号**：$i$ 和 $j$ 相当于表示为相同的平方过程。

### 用于召回的改进

## item2vec召回

### word2vec方法

- **one-hot编码的缺陷**：
  1. 矩阵稀疏，尤其是当特征数量很大的时候
  2. 构成的词向量为正交运算，无法通过数学运算计算词之间的相似性

- **word2Vec算法**：
  1. 使用大量文本预料库
  2. 构建的词汇表中，每一个单词都由宇哥词向量表示
  3. 遍历文本的每一个位置t，都有中心词c与上下文词o(outside)
  4. 再整个语料库上，使用数学方法，最大化o在单词c周围出现的概率，进而得到每一个单词的dense表示
  5. 迭代更新，直到达到最佳效果
   
  - 模型结构
  - 
  输入为1 X V维的one-hot向量（V为整个词汇表的长度，这个向量只有一个1值，其余为0值表示一个词），单隐藏层（隐藏层的维度为N，这里是一个超参数，也就是词向量的维度），输出为1 X V维的softmax层的模型。$W^{I}$为V X N的参数矩阵，$W^{O}$为N X V的参数矩阵

  <div align=center>
  <img src="http://ryluo.oss-cn-chengdu.aliyuncs.com/图片8.png" alt="在这里插入图片描述" style="zoom:50%;" /> 
  </div>

- **Skip-gram模型**
  - 目的：最大化单词o在单词c周围出现的概率，也就是\(arg max P(o|c)\).通过设置窗口大小来控制考虑的元素。具体如下如图所示。通过设置窗口为2，然后不断滑动窗口，也从而得到了所有位置元素的概率值。

  <div align=center>
  <img src="http://ryluo.oss-cn-chengdu.aliyuncs.com/图片3.png" alt="在这里插入图片描述" style="zoom:50%;" /> 
  </div>

  - 求解：极大似然法 $ max\prod_{c} \prod_{o}P(o|c) $， 也就是
  \(\prod_{t=1}^{T} \prod_{\substack{j=-m \\ j\neq0}}^{m} P(w_{t+j}|w_t;\theta)\), 
  对整体的函数取log，并加负号。得到最终的损失函数：\(\frac{1}{T} \sum_{t=1}^{T} \sum_{\substack{j=-m \\ j \neq 0}}^{m} \log p(w_{t+j}|w_t+\theta)\)
  
  - \(P(o|c)\)计算：使用中心词c与上下文词o的相似性计算，使用向量的点积表示$u_o \cdot v_c$，然后使用softmax函数映射成一个概率值，如下。每个词都会有两个词向量，一个是自己作为中心词的向量，一个是自己作为上下文词的向量。
  
  $$
  \frac{1}{T} \sum_{t=1}^{T} \sum_{\substack{j=-m \\ j \neq 0}}^{m} \log p(w_{t+j}|w_t+\theta)
  $$

  - 结构：
    ==给定中心词，预测周围的词==

  <div align=center>
  <img src="http://ryluo.oss-cn-chengdu.aliyuncs.com/图片image-20220424105817437.png" alt="在这里插入图片描述" style="zoom:50%;" /> 
  </div>

- **CBOW**
  - 模型结构
  
  == 给定周围词，预测中心词==

  <div align=center>
  <img src="http://ryluo.oss-cn-chengdu.aliyuncs.com/图片10.png" alt="在这里插入图片描述" style="zoom:50%;" /> 
  </div>

- **负采样**
  - softmax的问题：分母需要在整个单词表上计算乘积与指数运算，计算资源消耗很大。
  - 负采样：
  损失函数如下，其中$\sigma$为sigmoid函数$1/(1+e^{-x})$，$u_{o}$为实际样本中的上下文词的词向量，而$u_{k}$为按一定的规则随机选出的K个单词。由单调性规则，$u_{o}^T \cdot v_c$越大，损失函数越小，而$u_{k}^T \cdot v_c$越小，损失函数越大

  $$
  J_{neg_{sample}}(b_{u_o}, v_c, U) = -\log \sigma({u_o}^Tv_C) - \sum_{k\in K(\text{sampled indices})} \log \sigma(-u_k^Tv_C)
  $$
  - 单词 $w$ 作为负样本时，被采样到的概率：
  
  $$
  \frac{[\operatorname{counter}(w)]^{0.75}}{\sum_{u \in \mathcal{W}}[\operatorname{counter}(u)]^{0.75}}
  $$

  - 单词 $w$ 作为中心词时，被丢弃的概率：
  - 
  $$
  \operatorname{prob}(w)=1-\sqrt{\frac{t}{f(w)}}
  $$

  - 关于$v_c$的导数：

  $$ 
  \begin{aligned}
  \frac{\partial J_{\text{neg-sample}}(\boldsymbol v_c, o, \boldsymbol U)}{\partial \boldsymbol v_c} &= -\frac{\sigma(\boldsymbol u_o^T \boldsymbol v_c)(1 - \sigma(\boldsymbol u_o^T \boldsymbol v_c))}{\sigma(\boldsymbol u_o^T \boldsymbol v_c)} \frac{\partial (\boldsymbol u_o^T \boldsymbol v_c)}{\partial \boldsymbol v_c} - \sum_{k=1}^{K} \frac{\partial \log(\sigma(-\boldsymbol u_k^T \boldsymbol v_c))}{\partial \boldsymbol v_c}\\ &= -(1 - \sigma(\boldsymbol u_o^T \boldsymbol v_c)) \boldsymbol u_o + \sum_{k=1}^{K} (1 - \sigma(-\boldsymbol u_k^T \boldsymbol v_c)) \boldsymbol u_k
  \end{aligned}
  $$

### item2Vec召回

- **原理**：基于物品集合进行训练，丢弃物品的时间与空间信息。同时，假设对于一个集合的物品，它们之间是相似的，与用户购买它们的顺序、时间无关。

- **目标函数**：
$$
\frac{1}{K} \sum_{i=1}^{K} \sum_{j \neq i}^{K} \log p\left(w_{j} \mid w_{i}\right)
$$

- **相似度计算**：使用余弦相似度
- **问题**：计算不同物品之间的相似度时，仍然会依赖不同物品之间的共性，无法解决冷启动问题。一种解决方法是取出与冷启物品类别相同的非冷启物品，将它们向量的均值作为冷启动物品的向量表示。

## Airbnb召回

### Embedding方法
1. 用于描述短期实时性的个性化特征 Embedding：listing Embeddings（listing代表房源）
   - 学习来源：通过用户的点击对话学习特征，表示房源的短期实时特征。给定数据集 $ \mathcal{S} $ ，其中包含了 $ N $ 个用户的 $ S $ 个点击 session（序列）。

     - 每个 session $ s=\left(l_{1}, \ldots, l_{M}\right) \in \mathcal{S} $ ，包含了 $ M $ 个被用户点击过的 listing ids 。
     - 对于用户连续两次点击，若时间间隔超过了30分钟，则启动新的 session。
   - 优化：在得到多个用户点击的session之后，基于word2Vec的SkipGram模型学习不同的listing embedding表示。最大化目标函数：
    $$ 
    \mathcal{L}=\sum_{s \in \mathcal{S}} \sum_{l_{i} \in s}\left(\sum_{-m \geq j \leq m, i \neq 0} \log \mathbb{P}\left(l_{i+j} \mid l_{i}\right)\right) 
    $$
    其中，$\mathbb{P}\left(l_{i+j} \mid l_{i}\right)$ 基于softmax函数，表示一个session中，已知中心listing $l_i$ 来预测上下文 listing $l_{i+j}$ 的概率，也就是：
    $$ 
    \mathbb{P}\left(l_{i+j} \mid l_{i}\right)=\frac{\exp \left(\mathbf{v}{l{i}}^{\top} \mathbf{v}{l{i+j}}^{\prime}\right)}{\sum_{l=1}^{|\mathcal{V}|} \exp \left(\mathbf{v}{l{i}}^{\top} \mathbf{v}_{l}^{\prime}\right)} $$
    其中， $ \mathbf{v}{l{i}} $ 表示 listing $ l_i $ 的 Embedding 向量， $ |\mathcal{V}| $ 表示全部的物料库的数量。由于物料库过大，需要做负采样处理，负采样的目标函数为：
    $$ 
    \underset{\theta}{\operatorname{argmax}} \sum_{(l, c) \in \mathcal{D}{p}} \log \frac{1}{1+e^{-\mathbf{v}{c}^{\prime^{\prime}} \mathbf{v}{l}}}+\sum{(l, c) \in \mathcal{D}{n}} \log \frac{1}{1+e^{\mathbf{v}{c}^{\prime} \mathbf{v}_{l}}} 
    $$ 
    - 正负样本集构建
      - 使用 booked listing 作为全局上下文。booked listing 表示用户在 session 中最终预定的房源，一般只会出现在结束的 session 中。
      - Airbnb 将最终预定的房源，始终作为滑窗的上下文，即**全局上下文**。如下图，对于当前滑动窗口的 central listing，实线箭头表示context listings，虚线（指向booked listing）表示 global context listing。

    <img src="http://ryluo.oss-cn-chengdu.aliyuncs.com/图片1653053823336-0564b2da-c993-46aa-9b22-f5cbb784dae2.png" alt="img" style="zoom:50%;" />

    - booked listing 作为全局正样本，故优化的目标函数更新为：

    $$
    \underset{\theta}{\operatorname{argmax}} \sum_{(l, c) \in \mathcal{D}_{p}} \log \frac{1}{1+e^{-\mathbf{v}_{c}^{\prime^{\prime}} \mathbf{v}_{l}}}+\sum_{(l, c) \in \mathcal{D}_{n}} \log \frac{1}{1+e^{\mathbf{v}_{c}^{\prime} \mathbf{v}_{l}}} +
    \log \frac{1}{1+e^{-\mathbf{v}_{c}^{\prime} \mathbf{v}_{l_b}}}
    $$

    - 优化负样本的选择
      - 用户通过在线网站预定房间时，通常只会在同一个 market （将要停留区域）内进行搜索。
   
      - 对于用户点击过的样本集 $ \mathcal{D}_{p} $ （正样本集）而言，它们大概率位于**同一片区域**。考虑到负样本集 $ \mathcal{D}_{n} $ 是随机抽取的，大概率来源不同的区域。
   
      - Airbnb 发现这种样本的不平衡，在学习同一片区域房源的 Embedding 时会得到次优解。
   
      - 解决办法也很简单，对于每个滑窗中的中心 lisitng，其负样本的选择新增了**与其位于同一个 market 的 listing**。至此，优化函数更新如下：
     
    $$
    \underset{\theta}{\operatorname{argmax}} \sum_{(l, c) \in \mathcal{D}_{p}} \log \frac{1}{1+e^{-\mathbf{v}_{c}^{\prime^{\prime}} \mathbf{v}_{l}}}+\sum_{(l, c) \in \mathcal{D}_{n}} \log \frac{1}{1+e^{\mathbf{v}_{c}^{\prime} \mathbf{v}_{l}}} +\log \frac{1}{1+e^{-\mathbf{v}_{c}^{\prime} \mathbf{v}_{l_b}}} + 
    \sum_{(l, m_n ) \in \mathcal{D}_{m_n}} \log \frac{1}{1+e^{\mathbf{v}_{m_n}^{\prime} \mathbf{v}_{l}}}
    $$ 

      - $ \mathcal{D}_{m_n} $ 表示与滑窗中的中心 listing 位于同一区域的负样本集。
      - 冷启动：根据房源信息，查找最相似的三个房源，用三个房源的均值作为新的房源的embedding表示。

2. 用于描述长期的个性化特征 Embedding：user-type & listing type Embeddings
   - 潜在问题：
     - 需要基于booking session进行学习，但是数据量很小，因为booking本身就是低频率时间
     - 如果用户只预定过单个数量的房源，则无法从该会话中学习到embedding信息。因为要学习有效的embedding，该实体必须至少出现五次以上。
     - booking的时间跨度会比较大，无法保证用户在这么长的时间跨度里兴趣是否发生变化。
   - 改进：定一个 booking sessions 集合 $\mathcal{S}_{b}$ ，其中包含了$M$个用户的 booking session：每个booking session表示为：$s_{b}=\left(l_{b 1}, \ldots, l_{b M}\right)$，$l_{b1}$ 表示 listing_id，学习到 Embedding 记作 $\mathbf{v}{l{i d}}$。也就是兼顾用户的个人信息与房源信息。
   - 训练过程
     - 图例：
      <img src="http://ryluo.oss-cn-chengdu.aliyuncs.com/图片1653131985447-e033cb39-235b-4f46-9634-3b7faec284be.png" alt="img" style="zoom:50%;" />

    - 联合训练 User-type Embedding 和 Listing-type Embedding
      - 如图（a），在 booking session 中，每个元素代表的是 User-type, Listing-type组合。
      - 为了学习在相同向量空间中的 User-type 和 Listing-type 的 Embeddings，Airbnb 的做法是将 User-type 插入到 booking sessions 中。
      - 形成一个（User-type, Listing-type）组成的元组序列，这样就可以让 User-type 和 Listing-type 的在 session 中的相对位置保持一致了。
      
   - User-type 的目标函数：
      
      $$
      \underset{\theta}{\operatorname{argmax}} \sum_{\left(u_{t}, c\right) \in \mathcal{D}_{b o o k}} \log \frac{1}{1+e^{-\mathbf{v}_{c}^{\prime} \mathbf{v}_{u_{t}}}}+\sum_{\left(u_{t}, c\right) \in \mathcal{D}_{n e g}} \log \frac{1}{1+e^{\mathbf{v}_{c}^{\prime} \mathbf{v}_{u_{t}}}}
      $$
   
      +  $ \mathcal{D}_{\text {book }} $ 中的 $ u_t $ （中心词）表示 User-type， $ c $ （上下文）表示用户最近的预定过的 Listing-type。 $ \mathcal{D}_{\text {neg}} $ 中的 $ c $ 表示 negative Listing-type。
      +  $ u_t $ 表示 User-type 的 Embedding， $ \mathbf{v}_{c}^{\prime} $ 表示 Listing-type 的Embedding。

   - Listing-type 的目标函数：
     
     $$
     \begin{aligned}
     \underset{\theta}{\operatorname{argmax}} & \sum_{\left(l_{t}, c\right) \in \mathcal{D}_{b o o k}} \log \frac{1}{1+\exp ^{-\mathrm{v}_{c}^{\prime} \mathbf{v}_{l_{t}}}}+\sum_{\left(l_{t}, c\right) \in \mathcal{D}_{n e g}} \log \frac{1}{1+\exp ^{\mathrm{v}_{c}^{\prime} \mathbf{v}_{l_{t}}}} \\
     \end{aligned}
     $$

     + 同理，不过窗口中的中心词为 Listing-type， 上下文为 User-type。

- Explicit Negatives for Rejections
   - 用户预定房源以后，还要等待房源主人的确认，主人可能接受或者拒绝客人的预定。拒接的原因可能包括，客人星级评定不佳，资料不完整等。
      
   - 前面学习到的 User-type Embedding 包含了客人的兴趣偏好，Listing-type Embedding 包含了房源的属性特征。但是，用户的 Embedding 未包含更容易被哪类房源主人拒绝的潜语义信息房源的 Embedding 未包含主人对哪类客人的拒绝偏好。
      
   - 为了提高用户预定房源以后，被主人接受的概率。同时，降低房源主人拒绝客人的概率。Airbnb 在训练 User-type 和 Listing-type 的 Embedding时，将用户预定后却被拒绝的样本加入负样本集中（如图b）。
      - 更新后，Listing-type 的目标函数：
        
        $$
        \begin{aligned}
        \underset{\theta}{\operatorname{argmax}} & \sum_{\left(u_{t}, c\right) \in \mathcal{D}_{b o o k}} \log \frac{1}{1+\exp ^{-\mathbf{v}_{c}^{\prime} \mathbf{v}_{u_{t}}}}+\sum_{\left(u_{t}, c\right) \in \mathcal{D}_{n e g}} \log \frac{1}{1+\exp ^{\mathbf{v}_{c}^{\prime} \mathbf{v}_{u_{t}}}} \\
        &+\sum_{\left(u_{t}, l_{t}\right) \in \mathcal{D}_{\text {reject }}} \log \frac{1}{1+\exp ^{\mathrm{v}_{{l_{t}}}^{\prime} \mathrm{v}_{u_{t}}}} 
        \end{aligned}
        $$
      
      - 更新后，User-type 的目标函数：
        
        $$
        \begin{aligned}
        \underset{\theta}{\operatorname{argmax}} & \sum_{\left(l_{t}, c\right) \in \mathcal{D}_{b o o k}} \log \frac{1}{1+\exp ^{-\mathrm{v}_{c}^{\prime} \mathbf{v}_{l_{t}}}}+\sum_{\left(l_{t}, c\right) \in \mathcal{D}_{n e g}} \log \frac{1}{1+\exp ^{\mathrm{v}_{c}^{\prime} \mathbf{v}_{l_{t}}}} \\
        &+\sum_{\left(l_{t}, u_{t}\right) \in \mathcal{D}_{\text {reject }}} \log \frac{1}{1+\exp ^{\mathrm{v}^{\prime}_{u_{t}} \mathrm{v}_{l_{t}}}}
        \end{aligned}
        $$