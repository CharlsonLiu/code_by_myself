# LGBT + LR
+ 思想
  利用GBDT自动进行特征筛选和组合，进而生成新的离散特征向量，再把该特征向量当做LR模型的输入，来产生最后的预测结果，该模型能够综合利用用户、物品和上下文等多种不同的特征，生成较为全面的推荐结果。
+ GBDT模型
  GBDT是通过采用加法模型(即基函数的线性组合），以及不断减小训练过程产生的误差来达到将数据分类或者回归的算法， 其训练过程如下：

  <div align=center>
  <img src="https://img-blog.csdnimg.cn/20200908202508786.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3d1emhvbmdxaWFuZw==,size_1,color_FFFFFF,t_70#pic_center" style="zoom:65%;" />    
  </div>
  gbdt通过多轮迭代，每轮迭代会产生一个弱分类器，每个分类器**在上一轮分类器的残差基础上进行训练**。 gbdt对弱分类器的要求一般是足够简单，并且低方差高偏差。因为训练的过程是通过降低偏差来不断提高最终分类器的精度。由于上述高偏差和简单的要求，每个分类回归树的深度不会很深。最终的总分类器是将每轮训练得到的弱分类器加权求和得到的（也就是加法模型）。  
  GBDT 来解决二分类问题和解决回归问题的本质是一样的，都是通过不断构建决策树的方式，使预测结果一步步的接近目标值，但是二分类问题和回归问题的损失函数是不同的，回归问题中一般使用的是平方损失，而二分类问题中，GBDT和逻辑回归一样，使用的下面这个：

  $$
  L=\arg \min \left[\sum_{i}^{n}-\left(y_{i} \log \left(p_{i}\right)+\left(1-y_{i}\right) \log \left(1-p_{i}\right)\right)\right]
  $$
  其中， $y_i$是第$i$个样本的观测值， 取值要么是0要么是1， 而$p_i$是第$i$个样本的预测值， 取值是0-1之间的概率，由于知道GBDT拟合的残差是当前模型的负梯度， 那么需要求出这个模型的导数，即$\frac{dL}{dp_i}$，对于某个特定的样本，求导的话就可以只考虑它本身，去掉加和号，那么就变成了$\frac{dl}{dp_i}$，其中$l$如下：
  $$
  \begin{aligned}
  l &=-y_{i} \log \left(p_{i}\right)-\left(1-y_{i}\right) \log \left(1-p_{i}\right) \\
  &=-y_{i} \log \left(p_{i}\right)-\log \left(1-p_{i}\right)+y_{i} \log \left(1-p_{i}\right) \\
  &=-y_{i}\left(\log \left(\frac{p_{i}}{1-p_{i}}\right)\right)-\log \left(1-p_{i}\right)
  \end{aligned}
  $$
  根据逻辑回归，$\left(\log \left(\frac{p_{i}}{1-p_{i}}\right)\right)$就是对几率比取了个对数，并且在逻辑回归里面这个式子等于$\theta X$,所以才推出了$p_i=\frac{1}{1+e^-{\theta X}}$的那个形式。这里令$\eta_i=\frac{p_i}{1-p_i}$, 即$p_i=\frac{\eta_i}{1+\eta_i}$, 则上面这个式子变成了：
  $$
  \begin{aligned}
  l &=-y_{i} \log \left(\eta_{i}\right)-\log \left(1-\frac{e^{\log \left(\eta_{i}\right)}}{1+e^{\log \left(\eta_{i}\right)}}\right) \\
  &=-y_{i} \log \left(\eta_{i}\right)-\log \left(\frac{1}{1+e^{\log \left(\eta_{i}\right)}}\right) \\
  &=-y_{i} \log \left(\eta_{i}\right)+\log \left(1+e^{\log \left(\eta_{i}\right)}\right)
  \end{aligned}
  $$

  对$log(\eta_i)$求导，得
  $$
  \frac{d l}{d \log (\eta_i)}=-y_{i}+\frac{e^{\log \left(\eta_{i}\right)}}{1+e^{\log \left(\eta_{i}\right)}}=-y_i+p_i
  $$

  这样， 就得到了某个训练样本在当前模型的梯度值，那么残差就是$y_i-p_i$。GBDT二分类的这个思想，其实和逻辑回归的思想一样，**逻辑回归是用一个线性模型去拟合$P(y=1|x)$这个事件的对数几率$log\frac{p}{1-p}=\theta^Tx$**， GBDT二分类也是如此，用一系列的梯度提升树去拟合这个对数几率，其分类模型可以表达为：

  $$
  P(Y=1 \mid x)=\frac{1}{1+e^{-F_{M}(x)}}
  $$

  下面具体来看GBDT的生成过程， 构建分类GBDT的步骤有两个：
    1. 初始化GBDT
    和回归问题一样，分类 GBDT 的初始状态也只有一个叶子节点，该节点为所有样本的初始预测值，如下：
$$
F_{0}(x)=\arg \min _{\gamma} \sum_{i=1}^{n} L(y, \gamma)
$$

  	上式 $F$代表GBDT模型，$F_0$是模型的初识状态，该式子的意思是找到一个$\gamma$，使所有样本的 Loss 最小，在这里及下文中，$\gamma$都表示节点的输出，即叶子节点，且它是一个 $log(\eta_i)$ 形式的值(回归值)，在初始状态，$\gamma =F_0$。
  示例：
    <div align=center>
    <img src="https://img-blog.csdnimg.cn/20200910095539432.png#pic_center" alt="在这里插入图片描述" style="zoom:80%;" /> 
    </div>

 	希望构建 GBDT 分类树，它能通过「喜欢爆米花」、「年龄」和「颜色偏好」这 3 个特征来预测某一个样本是否喜欢看电影。把数据代入上面的公式中求Loss:
    $$
    \operatorname{Loss}=L(1, \gamma)+L(1, \gamma)+L(0, \gamma)
    $$
  		为了令其最小，求导让导数为0，则：
    $$
    \operatorname{Loss}=p-1 + p-1+p=0
    $$
 		 于是， 就得到了初始值$p=\frac{2}{3}=0.67, \gamma=log(\frac{p}{1-p})=0.69$, 模型的初识状态$F_0(x)=0.69$

    2. 循环生成决策树
    这里回忆一下回归树的生成步骤，其实有4小步，第一就是计算负梯度值得到残差，第二步是用回归树拟合残差，第三步是计算叶子节点的输出值，第四步是更新模型。 
        1. 计算负梯度得到残差
         $$
         r_{i m}=-\left[\frac{\partial L\left(y_{i}, F\left(x_{i}\right)\right)}{\partial F\left(x_{i}\right)}\right]_{F(x)=F_{m-1}(x)}
         $$
         此处使用$m-1$棵树的模型， 计算每个样本的残差$r_{im}$, 就是上面的$y_i-pi$, 于是例子中， 每个样本的残差：
         <div align=center>
         <img src="https://img-blog.csdnimg.cn/20200910101154282.png#pic_center" alt="在这里插入图片描述" style="zoom:80%;" />
         </div>
    
         2. 使用回归树来拟合$r_{im}$， 这里的$i$表示样本，简单的说就是遍历每个特征，每个特征下遍历每个取值，计算分裂后两组数据的平方损失，找到最小的那个划分节点。 假如我们产生的第2棵决策树如下：
    
         <div align=center>
         <img src="https://img-blog.csdnimg.cn/20200910101558282.png#pic_center" alt="在这里插入图片描述" style="zoom:80%;" />
         </div>
    
         3. 对于每个叶子节点$j$, 计算最佳残差拟合值
         $$
         \gamma_{j m}=\arg \min _{\gamma} \sum_{x \in R_{i j}} L\left(y_{i}, F_{m-1}\left(x_{i}\right)+\gamma\right)
         $$
         意思是，在刚构建的树$m$中，找到每个节点$j$的输出$\gamma_{jm}$, 能使得该节点的loss最小。看一下这个$\gamma$的求解方式.首先，把损失函数写出来，对于左边的第一个样本，有
         $$
         L\left(y_{1}, F_{m-1}\left(x_{1}\right)+\gamma\right)=-y_{1}\left(F_{m-1}\left(x_{1}\right)+\gamma\right)+\log \left(1+e^{F_{m-1}\left(x_{1}\right)+\gamma}\right)
         $$
         这个式子就是上面推导的$l$，因为要用回归树做分类，所以这里把分类的预测概率转换成了对数几率回归的形式，即$log(\eta_i)$，这个就是模型的回归输出值。而如果求这个损失的最小值要，求导解出令损失最小的$\gamma$。但是上面这个式子求导会很麻烦，所以这里介绍了一个技巧就是**使用二阶泰勒公式来近似表示该式，再求导**
         $$
         f(x+\Delta x) \approx f(x)+\Delta x f^{\prime}(x)+\frac{1}{2} \Delta x^{2} f^{\prime \prime}(x)+O(\Delta x)
         $$
         这里就相当于把$L(y_1, F_{m-1}(x_1))$当做常量$f(x)$， $\gamma$作为变量$\Delta x$， 将$f(x)$二阶展开：
         $$
         L\left(y_{1}, F_{m-1}\left(x_{1}\right)+\gamma\right) \approx L\left(y_{1}, F_{m-1}\left(x_{1}\right)\right)+L^{\prime}\left(y_{1}, F_{m-1}\left(x_{1}\right)\right) \gamma+\frac{1}{2} L^{\prime \prime}\left(y_{1}, F_{m-1}\left(x_{1}\right)\right) \gamma^{2}
         $$
         这时候再求导就简单了
         $$
         \frac{d L}{d \gamma}=L^{\prime}\left(y_{1}, F_{m-1}\left(x_{1}\right)\right)+L^{\prime \prime}\left(y_{1}, F_{m-1}\left(x_{1}\right)\right) \gamma
         $$
         Loss最小的时候， 上面的式子等于0， 就可以得到$\gamma$:
         $$
         \gamma_{11}=\frac{-L^{\prime}\left(y_{1}, F_{m-1}\left(x_{1}\right)\right)}{L^{\prime \prime}\left(y_{1}, F_{m-1}\left(x_{1}\right)\right)}
         $$
         **因为分子就是残差(上述已经求到了)， 分母可以通过对残差求导，得到原损失函数的二阶导：**
         $$
         \begin{aligned}
         L^{\prime \prime}\left(y_{1}, F(x)\right) &=\frac{d L^{\prime}}{d \log (\eta_1)} \\
         &=\frac{d}{d \log (\eta_1)}\left[-y_{i}+\frac{e^{\log (\eta_1)}}{1+e^{\log (\eta_1)}}\right] \\
         &=\frac{d}{d \log (\eta_1)}\left[e^{\log (\eta_1)}\left(1+e^{\log (\eta_1)}\right)^{-1}\right] \\
         &=e^{\log (\eta_1)}\left(1+e^{\log (\eta_1)}\right)^{-1}-e^{2 \log (\eta_1)}\left(1+e^{\log (\eta_1)}\right)^{-2} \\
         &=\frac{e^{\log (\eta_1)}}{\left(1+e^{\log (\eta_1)}\right)^{2}} \\
         &=\frac{\eta_1}{(1+\eta_1)}\frac{1}{(1+\eta_1)} \\
         &=p_1(1-p_1)
         \end{aligned}
         $$
         这时候， 就可以算出该节点的输出：
         $$
         \gamma_{11}=\frac{r_{11}}{p_{10}\left(1-p_{10}\right)}=\frac{0.33}{0.67 \times 0.33}=1.49
         $$
         这里的下面$\gamma_{jm}$表示第$m$棵树的第$j$个叶子节点。 接下来是右边节点的输出， 包含样本2和样本3， 同样使用二阶泰勒公式展开：
         $$
         \begin{array}{l}
         L\left(y_{2}, F_{m-1}\left(x_{2}\right)+\gamma\right)+L\left(y_{3}, F_{m-1}\left(x_{3}\right)+\gamma\right) \\
         \approx L\left(y_{2}, F_{m-1}\left(x_{2}\right)\right)+L^{\prime}\left(y_{2}, F_{m-1}\left(x_{2}\right)\right) \gamma+\frac{1}{2} L^{\prime \prime}\left(y_{2}, F_{m-1}\left(x_{2}\right)\right) \gamma^{2} \\
         +L\left(y_{3}, F_{m-1}\left(x_{3}\right)\right)+L^{\prime}\left(y_{3}, F_{m-1}\left(x_{3}\right)\right) \gamma+\frac{1}{2} L^{\prime \prime}\left(y_{3}, F_{m-1}\left(x_{3}\right)\right) \gamma^{2}
         \end{array}
         $$
         求导， 令其结果为0，就会得到， 第1棵树的第2个叶子节点的输出：
         $$
         \begin{aligned}
         \gamma_{21} &=\frac{-L^{\prime}\left(y_{2}, F_{m-1}\left(x_{2}\right)\right)-L^{\prime}\left(y_{3}, F_{m-1}\left(x_{3}\right)\right)}{L^{\prime \prime}\left(y_{2}, F_{m-1}\left(x_{2}\right)\right)+L^{\prime \prime}\left(y_{3}, F_{m-1}\left(x_{3}\right)\right)} \\
         &=\frac{r_{21}+r_{31}}{p_{20}\left(1-p_{20}\right)+p_{30}\left(1-p_{30}\right)} \\
         &=\frac{0.33-0.67}{0.67 \times 0.33+0.67 \times 0.33} \\
         &=-0.77
         \end{aligned}
         $$
         可以看出， 对于任意叶子节点， 我们可以直接计算其输出值：
         $$
         \gamma_{j m}=\frac{\sum_{i=1}^{R_{i j}} r_{i m}}{\sum_{i=1}^{R_{i j}} p_{i, m-1}\left(1-p_{i, m-1}\right)}
         $$
    
          4. 更新模型$F_m(x)$
         $$
         F_{m}(x)=F_{m-1}(x)+\nu \sum_{j=1}^{J_{m}} \gamma_{m}
         $$
    
    这样， 通过多次循环迭代， 就可以得到一个比较强的学习器$F_m(x)$
    
+ GDBT + LR
  + 基本结构
  <div align=center>
  <img src="https://img-blog.csdnimg.cn/20200910161923481.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3d1emhvbmdxaWFuZw==,size_1,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述" style="zoom:67%;" />    
  </div>
  训练时，GBDT 建树的过程相当于自动进行的特征组合和离散化，然后从根结点到叶子节点的这条路径就可以看成是不同特征进行的特征组合，用叶子节点可以唯一的表示这条路径，并作为一个离散特征传入 LR 进行**二次训练**。

+ 注意点
  1. **通过GBDT进行特征组合之后得到的==离散向量和训练数据的原特征一块==作为逻辑回归的输入，而不仅仅全是这种离散特征**
  2. 建树的时候用ensemble建树的原因就是一棵树的表达能力很弱，不足以表达多个有区分性的特征组合，多棵树的表达能力更强一些。GBDT每棵树都在学习前面棵树尚存的不足，迭代多少次就会生成多少棵树。
  3. RF也是多棵树，但从效果上有实践证明不如GBDT。且GBDT前面的树，特征分裂主要体现对多数样本有区分度的特征；后面的树，主要体现的是经过前N颗树，残差仍然较大的少数样本。优先选用在整体上有区分度的特征，再选用针对少数样本有区分度的特征，思路更加合理，这应该也是用GBDT的原因。
  4. 在CRT预估中，GBDT一般会建立两类树(非ID特征建一类， ID类特征建一类)，AD，ID类特征在CTR预估中是非常重要的特征，直接将AD，ID作为feature进行建树不可行，需要为每个AD，ID建GBDT树。
   
# 特征交叉

## FM模型（分解机）
* 优化函数

$$
y = w_0+\sum_{i=1}^nw_ix_i+\sum_{i=1}^{n}\sum_{i+1}^n\lt v_i,v_j\gt x_ix_j
$$

改进思想：**为每个 $x_i$ 计算一个 embedding，然后将两个向量之间的 embedding 做内积得到之前所谓的 $w_{ij}$**。这使得即使两个特征之前从未在训练集中**同时**出现，只需要 $x_i$ 和其他的 $x_k$ 同时出现过就可以计算出 $x_i$ 的 embedding，大大提升了模型的泛化能力。**当交叉项参数 $w_{ij}$ 全为 0 的时候，整个模型就退化为普通的 LR 模型**。

公式中各参数的意义：

- $\omega_{0}$：**全局偏置**；
- $\omega_{i}$：模型第 $i$ 个变量的权重；
- $\omega_{ij} = < v_{i}, v_{j}>$：特征 $i$ 和 $j$ 的交叉权重；
- $v_{i}$：第 $i$ 维特征的隐向量；
- $<\cdot, \cdot>$：向量点积；
- $k(k<<n)$：隐向量的长度，包含 $k$ 个描述特征的因子。

* 降低时间复杂度的证明

利用 $\sum$ 运算的线性性质，公式可以化简为：

$$
\begin{align} 
\sum_{i=1}^{n-1}{\sum_{j=i+1}^{n}{<v_i,v_j>x_ix_j}}
&= \frac{1}{2}\sum_{i=1}^{n}{\sum_{j=1}^{n}{<v_i,v_j>x_ix_j}} - \frac{1}{2} {\sum_{i=1}^{n}{<v_i,v_i>x_ix_i}} \\
&= \frac{1}{2} \left( \sum_{i=1}^{n}{\sum_{j=1}^{n}{\sum_{f=1}^{k}{v_{i,f}v_{j,f}x_ix_j}}} - \sum_{i=1}^{n}{\sum_{f=1}^{k}{v_{i,f}v_{i,f}x_ix_i}} \right) \\
&= \frac{1}{2}\sum_{f=1}^{k}{\left[ \left( \sum_{i=1}^{n}{v_{i,f}x_i} \right) \cdot \left( \sum_{j=1}^{n}{v_{j,f}x_j} \right) - \sum_{i=1}^{n}{v_{i,f}^2 x_i^2} \right]} \\
&= \frac{1}{2}\sum_{f=1}^{k}{\left[ \left( \sum_{i=1}^{n}{v_{i,f}x_i} \right)^2 - \sum_{i=1}^{n}{v_{i,f}^2 x_i^2} \right]} 
\end{align}
$$

* 解释

- $v_{i,f}$ 是一个具体的值；
- **第 1 个等号**：对称矩阵 $W$ 的对角线上半部分；
- **第 2 个等号**：将向量内积 $<v_i, v_j>$ 展开为累加和的形式；
- **第 3 个等号**：提出公共部分；
- **第 4 个等号**：$i$ 和 $j$ 相当于表示为相同的平方过程。

## Product Neural Network (PNN)

**工程应用**：通常仅用 IPNN（Inner Product Neural Network）。

---

**模型结构**

<div align="center">
  <img src="http://ryluo.oss-cn-chengdu.aliyuncs.com/图片image-20210308142624189.png" alt="PNN Model" style="zoom:50%;">
</div>

---

**原理**

- **线性模块**：将所有特征拼接在一起，论文中使用 1 进行线性变换。
- **非线性模块**：初始化参数矩阵表示特征，使用内积操作得到特征表示。公式如下：

$$
g(f_i,f_j) = <f_i, f_j>
$$

计算公式：

$$
l_p^n = \sum_{i=1}^N \sum_{j=1}^N (W_p^n)_{i,j} \langle f_i, f_j \rangle
$$

优化后的公式：

$$
l_p = (||\sum_{i=1}^N \theta^1 f_i||^2, ||\sum_{i=1}^N \theta^2 f_i||^2, ..., ||\sum_{i=1}^N \theta^{D_1} f_i||^2)
$$



## DCN
**改进：使用Cross Network替换掉了Wide部分，来自动进行特征之间的交叉，并且网络的时间和空间复杂度都是线性的。**
### 模型结构

<div align = center>
<img src="http://ryluo.oss-cn-chengdu.aliyuncs.com/图片dcn.png" style="zoom:67%;" />
</div>

### Embedding和Stacking层
这里的作用依然是把稀疏离散的类别型特征变成低维密集型。
$$
\mathbf{x}_{\text {embed, } i}=W_{\text {embed, } i} \mathbf{x}_{i}
$$

其中对于某一类稀疏分类特征（如id），$X_{embed, i}$是第个$i$分类值（id序号）的embedding向量。$W_{embed,i}$是embedding矩阵， $n_e\times n_v$维度， $n_e$是embedding维度， $n_v$是该类特征的唯一取值个数。$x_i$属于该特征的二元稀疏向量(one-hot)编码的。**实质上就是在训练得到的Embedding参数矩阵中找到属于当前样本对应的Embedding向量**。

最后，该层需要将所有的密集型特征与通过embedding转换后的特征进行联合（Stacking）：
$$
\mathbf{x}_{0}=\left[\mathbf{x}_{\text {embed, } 1}^{T}, \ldots, \mathbf{x}_{\text {embed, }, k}^{T}, \mathbf{x}_{\text {dense }}^{T}\right]
$$
一共$k$个类别特征， dense是数值型特征， 两者在特征维度拼在一块。 

### Cross Network
设计该网络的目的是增加特征之间的交互力度。交叉网络由多个交叉层组成， 假设第$l$层的输出向量$x_l$， 那么对于第$l+1$层的输出向量$x_{l+1}$表示为：

$$
\mathbf{x}_{l+1}=\mathbf{x}_{0} \mathbf{x}_{l}^{T} \mathbf{w}_{l}+\mathbf{b}_{l}+\mathbf{x}_{l}=f\left(\mathbf{x}_{l}, \mathbf{w}_{l}, \mathbf{b}_{l}\right)+\mathbf{x}_{l}
$$
可以看到， 交叉层的二阶部分非常类似PNN提到的外积操作， 在此基础上增加了外积操作的权重向量$w_l$， 以及原输入向量$x_l$和偏置向量$b_l$。 交叉层的可视化如下：

<img src="http://ryluo.oss-cn-chengdu.aliyuncs.com/图片cross.png" style="zoom:67%;" />

可以看到， 每一层增加了一个$n$维的权重向量$w_l$（n表示输入向量维度）， 并且在每一层均保留了输入向量， 因此输入和输出之间的变化不会特别明显。关于这一层， 原论文里面有个具体的证明推导Cross Network为啥有效， 不过比较复杂，这里我拿一个式子简单的解释下上面这个公式的伟大之处：

> **我们根据上面这个公式， 尝试的写前面几层看看:**
>
> $l=0:\mathbf{x}_{1} =\mathbf{x}_{0} \mathbf{x}_{0}^{T} \mathbf{w}_{0}+ \mathbf{b}_{0}+\mathbf{x}_{0}$
>
> $l=1:\mathbf{x}_{2} =\mathbf{x}_{0} \mathbf{x}_{1}^{T} \mathbf{w}_{1}+ \mathbf{b}_{1}+\mathbf{x}_{1}=\mathbf{x}_{0} [\mathbf{x}_{0} \mathbf{x}_{0}^{T} \mathbf{w}_{0}+ \mathbf{b}_{0}+\mathbf{x}_{0}]^{T}\mathbf{w}_{1}+\mathbf{b}_{1}+\mathbf{x}_{1}$
>
> $l=2:\mathbf{x}_{3} =\mathbf{x}_{0} \mathbf{x}_{2}^{T} \mathbf{w}_{2}+ \mathbf{b}_{2}+\mathbf{x}_{2}=\mathbf{x}_{0} [\mathbf{x}_{0} [\mathbf{x}_{0} \mathbf{x}_{0}^{T} \mathbf{w}_{0}+ \mathbf{b}_{0}+\mathbf{x}_{0}]^{T}\mathbf{w}_{1}+\mathbf{b}_{1}+\mathbf{x}_{1}]^{T}\mathbf{w}_{2}+\mathbf{b}_{2}+\mathbf{x}_{2}$

我们暂且写到第3层的计算，有以下结论：
1. $\mathrm{x}_1$中包含了所有的$\mathrm{x}_0$的1,2阶特征的交互， $\mathrm{x}_2$包含了所有的$\mathrm{x}_1, \mathrm{x}_0$的1、2、3阶特征的交互，$\mathrm{x}_3$中包含了所有的$\mathrm{x}_2$, $\mathrm{x}_1$与$\mathrm{x}_0$的交互，$\mathrm{x}_0$的1、2、3、4阶特征交互。 因此， 交叉网络层的叉乘阶数是有限的。 **第$l$层特征对应的最高的叉乘阶数$l+1$**

2. Cross网络的参数是共享的， 每一层的这个权重特征之间共享， 这个可以使得模型泛化到看不见的特征交互作用， 并且对噪声更具有鲁棒性。 例如两个稀疏的特征$x_i,x_j$， 它们在数据中几乎不发生交互， 那么学习$x_i,x_j$的权重对于预测没有任何的意义。

3. 计算交叉网络的参数数量。 假设交叉层的数量是$L_c$， 特征$x$的维度是$n$， 那么总共的参数是：

   $$
   n\times L_c \times 2
   $$
   这个就是每一层会有$w$和$b$。且$w$维度和$x$的维度是一致的。

4. 交叉网络的时间和空间复杂度是线性的。这是因为， 每一层都只有$w$和$b$， 没有激活函数的存在，相对于深度学习网络， 交叉网络的复杂性可以忽略不计。

5. Cross网络是FM的泛化形式， 在FM模型中， 特征$x_i$的权重$v_i$， 那么交叉项$x_i,x_j$的权重为$<x_i,x_j>$。在DCN中， $x_i$的权重为${W_K^{(i)}}_{k=1}^l$, 交叉项$x_i,x_j$的权重是参数${W_K^{(i)}}_{k=1}^l$和${W_K^{(j)}}_{k=1}^l$的乘积，这个看上面那个例子展开感受下。因此两个模型都各自学习了独立于其他特征的一些参数，并且交叉项的权重是相应参数的某种组合。FM只局限于2阶的特征交叉(一般)，而DCN可以构建更高阶的特征交互， 阶数由网络深度决定，并且交叉网络的参数只依据输入的维度线性增长。

6. 还有一点我们也要了解，对于每一层的计算中， 都会跟着$\mathrm{x}_0$, 这个是咱们的原始输入， 之所以会乘以一个这个，是为了保证后面不管怎么交叉，都不能偏离我们的原始输入太远，别最后交叉交叉都跑偏了。

7. $\mathbf{x}_{l+1}=f\left(\mathbf{x}_{l}, \mathbf{w}_{l}, \mathbf{b}_{l}\right)+\mathbf{x}_{l}$, 这个东西其实有点跳远连接的意思，也就是和ResNet也有点相似，无形之中还能有效的缓解梯度消失现象。

### 组合输出层
这个层负责将两个网络的输出进行拼接， 并且通过简单的Logistics回归完成最后的预测：
$$
p=\sigma\left(\left[\mathbf{x}_{L_{1}}^{T}, \mathbf{h}_{L_{2}}^{T}\right] \mathbf{w}_{\text {logits }}\right)
$$
其中$\mathbf{x}_{L_{1}}^{T}$$\mathbf{h}_{L_{2}}^{T}$表示交叉网络和深度网络的输出。
最后二分类的损失函数依然是交叉熵损失：
$$
\text { loss }=-\frac{1}{N} \sum_{i=1}^{N} y_{i} \log \left(p_{i}\right)+\left(1-y_{i}\right) \log \left(1-p_{i}\right)+\lambda \sum_{l}\left\|\mathbf{w}_{i}\right\|^{2}
$$

### 缺点
1. 每个隐藏层是$x_0$的标量倍，因此CrossNet的输出受特定形式的限制。证明如下：
>当 k = 1:
  $$
  \begin{aligned}
  X_1 &= X_0 (X_0^T W_1) + X_0 \\
  &= X_0 (X_0^T W_1 + 1) \\
  &= \alpha^1 X_0
  \end{aligned}
  $$
  这里的 $\alpha^1 = X_0^T W_1 + 1$ 是$X_0$的一个线性回归，$x_1$是$x_0$的标量倍成立。 假设 当$k=i$的时候也成立，那么$k=i+1$的时候：
  $$
  \begin{aligned}
  X_{i+1} &= X_0 X_i^T W_{i+1} + X_i \\
  &= X_0 (\alpha^i X_0)^T W_{i+1} + \alpha^i X_0 \\
  &= \alpha^{i+1} X_0
  \end{aligned}
  $$
  其中$\alpha^{i+1} = \alpha^i (X_0^T W_{i+1} + 1)$, 即$x_{i+1}$依然是$x_0$的标量倍。
  
2. 特征交互方式为bit-wise。这样会在泛化能力上造成限制，而且**意识不到Field Vector的概念**。

## DeepCrossing

**简介**：第一个完整解决特征工程、稀疏向量稠密化、和多层神经网络优化的推荐系统深度学习应用，主要用于点击率预测。

### 模型结构
<div align="center">
  <img src="http://ryluo.oss-cn-chengdu.aliyuncs.com/图片2020100916594542.png" alt="DeepCrossing Model" style="zoom:67%;">
</div>

### 模型原理
- **需解决问题**：
  1. 离散特征编码后过于稀疏，影响神经网络训练。
  2. 特征的自动交叉组合。
  3. 输出层达成优化目标。

- **各层作用**：
  1. **Embedding Layer**：将离散变量转换为OneHot变量，并转换为稠密向量。
  2. **Stacking Layer**：拼接数值型特征和Embedding作为模型输入。
  3. **MLP + Residual Connection**：两层MLP进行特征非线性变换，并与原始输入特征残差连接，最后线性层+激活函数得到预测。

# Wide & Deep系列

## Wide & Deep
**特性**：Wide部分有助于模型记忆，Deep部分有助于泛化，适合探索新特征组合。
**缺点**： Wide部分的特征组合需要建立在大量工程经验的基础上，极大的依赖人工处理。

### 模型结构
<div align="center">
  <img src="http://ryluo.oss-cn-chengdu.aliyuncs.com/Javaimage-20200910214310877.png" alt="Wide & Deep Model" style="zoom:65%;">
</div>

### 模型原理
- **Wide部分**：广义线性模型，包含原始特征和交叉特征，使用L1正则的FTRL优化，确保Wide部分稀疏化。
- **Deep部分**：DNN模型，包含数值特征和类别特征（需Embedding后输入DNN），通过非线性变换提高模型泛化能力。

**Wide与Deep结合**：通过逻辑回归联合训练Wide和Deep的输出，优化器使用FTRL（Wide部分）和Adagrad（Deep部分）：
$$
P(Y=1|x)=\delta(w_{wide}^T[x,\phi(x)] + w_{deep}^T a^{(lf)} + b)
$$

## 改进Wide侧
### DeepFM

#### 模型结构
<div align = center>
<img src="https://i-blog.csdnimg.cn/blog_migrate/90a21d85810a74600f340be652d209b2.png#pic_center" style="zoom:50%;" />
</div>

#### 模型原理
- **FM层**：这个在推荐系统中的地位非常高，需要重点关注。实际上就是一阶特征和二阶特征拼接之后经过sigmoid激活函数得到logits。
$$
\hat{y}_{FM}(x) = w_0+\sum_{i=1}^N w_ix_i + \sum_{i=1}^N \sum_{j=i+1}^N v_i^T v_j x_ix_j
$$
具体的结构如下：
<div align = center>
<img src="http://ryluo.oss-cn-chengdu.aliyuncs.com/图片image-20210225181340313.png" alt="image-20210225181340313" style="zoom: 67%;" />
</div>
- **Deep**：
结构图：
<div align = center>
<img src="http://ryluo.oss-cn-chengdu.aliyuncs.com/图片image-20210225181010107.png" alt="image-20210225181010107" style="zoom:50%;" />
</div>
用来学习高阶的特征组合，也就是使用MLP对拼接后的稠密特征进行处理。

---

### xDeepFM
==更有效的高阶显性交叉特征、更高的泛化能力，进而显性和隐性高阶特征的组合==
- **bit-wise VS vector-wise**   
  假设隐向量的维度是3维， 如果两个特征对应的向量分别是$(a_1, b_1, c_1)$和$(a_2,b_2, c_2)$

  - 交互方式1->bit-wise（element-wise）： 此时特征交互发生在元素级别上，在向量的每一位上交互，且学习权重$W_i$。也就是:$f(w_1a_1a_2, w_2b_1b_2,w_3c_1c_2)$. ==可能增加过拟合的风险，并失去一定的泛化能力==
  - 交互方式2->vector-wise：此时特征交互是发生在整个向量上的，交互的最小单元是整个向量，在交互完之后学习一个统一的权重$w$。==这个更加符合FM的特征-特征交互==

#### 模型架构
<div align=center> 
<img src="https://img-blog.csdnimg.cn/2021050520373226.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3d1emhvbmdxaWFuZw==,size_1,color_FFFFFF,t_70#pic_center" alt="image-20210308142624189" style="zoom: 60%;" /> 
</div>
- **CIN**部分：

  - 计算公式

  $$
  \mathbf{X}_{h, *}^{k}=\sum_{i=1}^{H_{k-1}} \sum_{j=1}^{m} \mathbf{W}_{i j}^{k, h}\left(\mathbf{X}_{i, *}^{k-1} \circ \mathbf{X}_{j, *}^{0}\right)
  $$
  - 约定：

    1. $\mathbf{X}^{0} \in \mathbb{R}^{m \times D}$: 这个就是输入，也就是embedding层的输出，可以理解为各个embedding的堆叠而成的矩阵，假设有$m$个特征，embedding的维度是$D$维，那么这样就得到了这样的矩阵， $m$行$D$列。$\mathbf{X}_{i, *}^{0}=\mathbf{e}_{i}$，这个表示第$i$个特征的embedding向量$e_i$。上标在这里表示的是网络的层数，输入可以看做第0层，下标表示的第几行的embedding向量，这个清楚了。
    2. $\mathbf{X}^{k} \in \mathbb{R}^{H_{k} \times D}$: 这个表示的是CIN网络第$k$层的输出，和上面这个一样，也是一个矩阵，每一行是一个embedding向量，每一列代表一个embedding维度。这里的$H_k$表示的是第$k$层特征的数量，也可以理解为神经元个数。那么显然，这个$\mathbf{X}^{k}$就是$H_k$个$D$为向量堆叠而成的矩阵。$\mathbf{X}_{h, *}^{k}$代表的就是第$k$层第$h$个特征向量了。

  计算方式可以参照下图。也就是：1. 计算特征之间的组合，也就是哈达玛积；2. 计算加权之后的特征，并求和。得到新的一层特征的embedding。

  <div align=center> 
  <img src="https://img-blog.csdnimg.cn/20210505215026537.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3d1emhvbmdxaWFuZw==,size_1,color_FFFFFF,t_70#pic_center" alt="image-20210308142624189" style="zoom: 60%;" /> 
  </div>

  - 从CNN角度理解
    先上示意图:
    <div align=center> 
    <img src="https://img-blog.csdnimg.cn/20210505221222945.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3d1emhvbmdxaWFuZw==,size_1,color_FFFFFF,t_70#pic_center" alt="image-20210308142624189" style="zoom: 60%;" /> 
    </div>
    其中，tensor$Z^{k+1}$表示$X^k$与$X^0$的外积，进而可以看成一张三维的图片，特征数为$H_{k-1}$,每个特征有D个通道，每个特征在每个通道上有m个向量作为表征。使用过滤器$W^{k,h}:H_{k-1} \times m$作为所有通道的共同的卷积核，卷积后沿着m轴的方向求和，作为新的特征表示。

    卷积核的大小与最后的卷积结果的格式的计算公式见基础知识。
  
  - Sum pooling（高阶特征交互）

    首先，对第k层输出的特征向量，只能用到上一层的输出，并没有像DCN的cross部分用到之前所有的信息交互。所以将每一层的特征向量沿着维度D进行加和，也就是该特征的embedding求和取代原有的向量，得到$H_k$的向量。然后将所有层的向量拼接，作为CIN部分的输出。
    
  - CIN核心图
  <div align=center> 
  <img src="https://img-blog.csdnimg.cn/2021050520530391.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3d1emhvbmdxaWFuZw==,size_1,color_FFFFFF,t_70#pic_center" alt="image-20210308142624189" style="zoom: 60%;" /> 
  </div>
  
---
## 改进Deep侧
### NFM

#### DeepFM上的改进思路
1. 改进思路->**用一个表达能力更强的函数来替代原FM中二阶隐向量内积的部分**，因为原本的二阶交叉是一个线性模型，非常局限。这里作者考虑将线性内积改变为神经网络进行特征交叉。
2. 计算公式
$$
\hat{y}_{N F M}(\mathbf{x})=w_{0}+\sum_{i=1}^{n} w_{i} x_{i}+f(\mathbf{x})
$$
#### 模型结构
1. 结构
<div align = center>
<img src = 'https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/2f40bbac85124a7db8e0c9bc3a3b2072~tplv-k3u1fbpfcp-zoom-in-crop-mark:1512:0:0:0.awebp' style="zoom:80%;" />
</div>

2. 输入层
   和之前处理一样，先onehot再稠密embedding

3. 特征交叉池化层
   实现了FM与DNN的无缝连接， 组成了一个大的网络，且能够正常的反向传播。假设$\mathcal{V}_{x}$是所有特征embedding的集合，在特征交叉池化层的操作：

  $$
  f_{B I}\left(\mathcal{V}_{x}\right)=\sum_{i=1}^{n} \sum_{j=i+1}^{n} x_{i} \mathbf{v}_{i} \odot x_{j} \mathbf{v}_{j}
  $$

  $\odot$表示两个向量的元素积操作，即两个向量对应维度相乘得到的元素积向量（不是点乘），其中第$k$维的操作：
  $$
  \left(v_{i} \odot v_{j}\right)_{k}=\boldsymbol{v}_{i k} \boldsymbol{v}_{j k}
  $$

  这便定义了在embedding空间特征的二阶交互，这个不仔细看会和感觉FM的最后一项很像，但是不一样，一定要注意这个地方不是两个隐向量的内积，而是**元素积(逐元素相乘)**，也就是这一个交叉完了之后**k个维度不求和**，最后会得到一个$k$维向量。在进行两两Embedding元素积之后，对交叉特征向量取和，得到该层的输出向量，很显然，输出是一个$k$维的向量。这里不求和的原因是因为**原本的FM里面的求和操作是一个线性操作，这会使得模型对特征的学习程度不够，作者想使用非线性的方法去综合特征，在这里就是使用深度学习方法做非线性特征变换**

  这里很大的一点改进就是加入特征池化层之后，把二阶交互的信息合并，且上面接了一个DNN网络，这样就能够增强FM的表达能力了，因为FM只能到二阶，而这里的DNN可以进行多阶且非线性，只要FM把二阶的学习好了，DNN这块学习来会更加容易， 作者在论文中也说明了这一点，且通过后面的实验证实了这个观点。

  如果不加DNN，NFM就退化成了FM，所以改进的关键就在于加了一个这样的层，组合了一下二阶交叉的信息，然后又给了DNN进行高阶交叉的学习，成了一种“加强版”的FM。

  Bi-Interaction层不需要额外的模型学习参数，更重要的是它在一个线性的时间内完成计算，和FM一致的，即时间复杂度为$O\left(k N_{x}\right)$，$N_x$为embedding向量的数量。参考FM，可以将上式转化为：
  $$
  f_{B I}\left(\mathcal{V}_{x}\right)=\frac{1}{2}\left[\left(\sum_{i=1}^{n} x_{i} \mathbf{v}_{i}\right)^{2}-\sum_{i=1}^{n}\left(x_{i} \mathbf{v}_{i}\right)^{2}\right]
  $$
  后面代码复现NFM就是用的这个公式直接计算，比较简便且清晰。可以看到与FM的差异就是少了一个对embedding维度求和的部分。
  
4. 隐藏层
  这一层就是全连接的神经网络， DNN在进行特征的高层非线性交互上有着天然的学习优势，公式如下：
  $$
  \begin{aligned} 
  \mathbf{z}_{1}=&\sigma_{1}\left(\mathbf{W}_{1} f_{B I} 
  \left(\mathcal{V}_{x}\right)+\mathbf{b}_{1}\right)  \\
  \mathbf{z}_{2}=& \sigma_{2}\left(\mathbf{W}_{2} \mathbf{z}_{1}+\mathbf{b}_{2}\right) \\
  \ldots \ldots \\
  \mathbf{z}_{L}=& \sigma_{L}\left(\mathbf{W}_{L} \mathbf{z}_{L-1}+\mathbf{b}_{L}\right)
  \end{aligned}
  $$
  这里的$\sigma_i$是第$i$层的激活函数。

5. 预测层

这个就是最后一层的结果直接过一个隐藏层，但注意由于这里是回归问题，没有加sigmoid激活：
$$
f(\mathbf{x})=\mathbf{h}^{T} \mathbf{z}_{L}
$$

所以， NFM模型的前向传播过程总结如下：
$$
\begin{aligned}
\hat{y}_{N F M}(\mathbf{x}) &=w_{0}+\sum_{i=1}^{n} w_{i} x_{i} \\
&+\mathbf{h}^{T} \sigma_{L}\left(\mathbf{W}_{L}\left(\ldots \sigma_{1}\left(\mathbf{W}_{1} f_{B I}\left(\mathcal{V}_{x}\right)+\mathbf{b}_{1}\right) \ldots\right)+\mathbf{b}_{L}\right)
\end{aligned}
$$

#### 创新点

特征交叉池化层，基于它，实现了FM和DNN的无缝连接，使得DNN可以在底层就学习到包含更多信息的组合特征，这时候，就会减少DNN的很多负担，只需要很少的隐藏层就可以学习到高阶特征信息。NFM相比之前的DNN， 


### AFM
** 改进方面：FM中没有考虑到不同特征交叉的重要性，权重均为1.通过加入注意力机制，为不同特征组合分配不同的权重**
#### 模型原理
<div align = center>
<img src="http://ryluo.oss-cn-chengdu.aliyuncs.com/图片image-20210131092744905.png" alt="image-20210131092744905" style="zoom: 50%;" />
</div>

#### Pair-wise Interaction Layer
AFM二阶交叉项(无attention)：所有非零特征对应的隐向量两两对应元素乘积，然后再向量求和，输出的还是一个向量。
$$
\sum_{i=1}^{n}\sum_{i+1}^n (v_i \odot v_j) x_ix_j
$$
特征的embedding可以表示为：$\varepsilon = {v_ix_i}$，经过Pair-wise Interaction Layer输出可得：
$$
f_{PI}(\varepsilon)=\{(v_i \odot v_j) x_ix_j\}_{i,j \in R_x}
$$
$R_x$表示的是有效特征集合。此时的$f_{PI}(\varepsilon)$表示的是一个向量集合，所以需要先将这些向量集合聚合成一个向量，然后在转换成一个数值：
$$
\hat{y} = p^T \sum_{(i,j)\in R_x}(v_i \odot v_j) x_ix_j + b
$$
上式中的求和部分就是将向量集合聚合成一个维度与隐向量维度相同的向量，通过向量$p$再将其转换成一个数值，b表示的是偏置。
#### Attention-based Pooling

简单的叙述一下使用MLP实现注意力机制的计算。假设现在有n个交叉特征(假如维度是k)，将nxk的数据输入到一个kx1的全连接网络中，输出的张量维度为nx1，使用softmax函数将nx1的向量的每个维度进行归一化，得到一个新的nx1的向量，这个向量所有维度加起来的和为1，每个维度上的值就可以表示原nxk数据每一行(即1xk的数据)的权重。用公式表示为：
$$
\alpha_{ij}' = h^T ReLU(W(v_i \odot v_j)x_ix_j + b)
$$
使用softmax归一化可得：
$$
\alpha_{ij} = \frac{exp(\alpha_{ij}')}{\sum_{(i,j)\in R_x}exp(\alpha_{ij}')}
$$
这样就得到了AFM二阶交叉部分的注意力权重，如果将AFM的一阶项写在一起，AFM模型用公式表示为：
$$
\hat{y}_{afm}(x) = w_0+\sum_{i=1}^nw_ix_i+p^T \sum_{(i,j)\in R_x}\alpha_{ij}(v_i \odot v_j) x_ix_j + b
$$

# 序列模型

## DIN模型

==在广告推荐中，会有 *大量的用户历史行为信息* ，DIN的创新点就是使用注意力机制对用户的兴趣做动态模拟，利用先前的重要的历史行为信息做预测==

### 特征表示
工业的CTR预测数据集一般是multi-group categorial form 的形式。如下图所示。
<div align=center>
<img src="https://img-blog.csdnimg.cn/20210118190044920.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3d1emhvbmdxaWFuZw==,size_1,color_FFFFFF,t_70#pic_center" style="zoom: 67%;" />
</div>
红框中包含了丰富的用户信息，也是需要重点关注的地方。例如，用户的信息可能是`[weekday=Friday, gender=Female, visited_cate_ids={Bag,Book}, ad_cate_id=Book]`.正常就是onehot编码，但是visited_cate_ids列表中，每个用户的特征长度是不一样的，会出现multi-hot的情况。
<div align=center>
<img src="https://img-blog.csdnimg.cn/20210118185933510.png" style="zoom:67%;" />
</div>
这也是输出模型的数据格式。同时特征不做任何的特征交互，这由神经网络去自主学习。

### 基线模型

基线模型为Embedding&MLP，DIN是在这个基础上加入了**注意力网络**来学习当前候选广告与用户历史行为特征之间的相关性，从而动态捕捉用户兴趣。具体结构如下：
<div align=center>
<img src="https://img-blog.csdnimg.cn/20210118191224464.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3d1emhvbmdxaWFuZw==,size_1,color_FFFFFF,t_70#pic_center" style="zoom:80%;" />
</div>

- **Embedding Layer**：将高维稀疏的输入转成低维稠密向量。每个离散特征下面都会对应着一个embedding词典， 维度是$D \times K$，这里的$D$表示的是隐向量的维度->每个类别被映射到的稠密向量的长度（embedding_dim），而$K$表示的是当前离散特征的唯一取值个数，也就是离散特征的取值总数（onehot向量长度）。

- **Pooling Layer and Concat layer**: pooling层的作用是将用户的历史行为embedding这个最终变成一个定长的向量，因为用户的历史购买记录不一样，在输入到全连接网络之前需要变成定长输入。也就是这个公式：
$$
e_i=pooling(e_{i1}, e_{i2}, ...e_{ik})
$$
Concat layer层的作用就是拼接，把所有的特征embedding向量，如果再有连续特征的话也算上，从特征维度拼接整合，作为MLP的输入。

- **MLP**:全连接
- **Loss**：二分类任务，用负对数似然。
$$
L=-\frac{1}{N} \sum_{(\boldsymbol{x}, y) \in \mathcal{S}}(y \log p(\boldsymbol{x})+(1-y) \log (1-p(\boldsymbol{x})))
$$

### 基于Base Model 的改进
==在基线模型的基础上加入了注意力机制以学习用户兴趣与当前候选广告间的关联程度。具体结构如下
<div align=center>
<img src="https://img-blog.csdnimg.cn/20210118220015871.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3d1emhvbmdxaWFuZw==,size_1,color_FFFFFF,t_70#pic_center" style="zoom: 80%;" />
</div>
改进的点是local activation unit，是一个前馈神经网络，输入是用户历史行为商品和当前的候选商品，输出是之间的相关性，也就是每个历史商品的权重，把这个权重与原来的历史行为embedding相乘求和就得到了用户的兴趣表示$\boldsymbol{v}_{U}(A)$, 计算公式如下：
$$
\boldsymbol{v}_{U}(A)=f\left(\boldsymbol{v}_{A}, \boldsymbol{e}_{1}, \boldsymbol{e}_{2}, \ldots, \boldsymbol{e}_{H}\right)=\sum_{j=1}^{H} a\left(\boldsymbol{e}_{j}, \boldsymbol{v}_{A}\right) \boldsymbol{e}_{j}=\sum_{j=1}^{H} \boldsymbol{w}_{j} \boldsymbol{e}_{j}
$$
这里的$\{\boldsymbol{v}_{A}, \boldsymbol{e}_{1}, \boldsymbol{e}_{2}, \ldots, \boldsymbol{e}_{H}\}$是用户$U$的历史行为特征embedding，$v_{A}$表示的是候选广告$A$的embedding向量，$a(e_j, v_A)=w_j$表示权重或者历史行为商品与当前广告$A$的相关性程度。$a(\cdot)$表示的上面那个前馈神经网络，也就是那个所谓的注意力机制， 当然，看图里的话，输入除了历史行为向量和候选广告向量外，还加了一个外积操作，有利于模型相关性建模的显性知识。