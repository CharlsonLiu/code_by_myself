# 基础知识
## Logistic回归

+ sigmoid公式：
$$
f(x)=\frac{1}{1+e^{-x}}
$$

+ sigmoid函数图像

<img src="http://ryluo.oss-cn-chengdu.aliyuncs.com/图片Sigmoid_function.png" alt="Sigmoid function" style="display: block; margin: 0 auto;">

+ 导数
\[
\sigma'(x) = \sigma(x) \cdot (1 - \sigma(x))
\]

+ LR分类器 ==二分类==
+ 表达式
$$
p_i(y_i=1 \mid x_i) = \frac{1}{1+e^{-w x_i}} \\ \\
p_i(y_i=0 \mid x_i) = \frac{e^{-w x_i}}{1+e^{-w x_i}}
$$
    其中，随机变量$x_i \in \mathbb{R}^{n}$为实数，随机变量$y_i$的取值为$\{0, 1\}$，参数$w\in \mathbb{R}^{n}$。$wx_i$表示变量$x_i$与参数$w$之间的内积。

+ 几率 ： ==事件发生概率与不发生的概率的比值
$$
\eta_{i}=\frac{p_{i}}{1-p_{i}}
$$

    将等式两边化为以`e`为底的指数函数，有：
$$
e^{wx_i} =\frac{p_{i}}{1-p_{i}}
\\ \Downarrow
\\
p_i = \frac{1}{1+e^{-w x_i}}
$$
    逻辑回归本质上就是**关于事件几率的线性回归**。当然，逻辑回归和线性回归存在本质上的不同，其次在损失函数上，线性回归中的损失函数是均方误差，而 Logistic 回归的损失函数是**负对数似然（Negative Log-Likelihood）**。

+ 假设

    逻辑回归模型的基本假设是$y_i$服从伯努利分布，也称为两点分布或者$0-1$分布。即：
$$
p_i(y_i=1 \mid x_i ; w)=p_i \\
p_i(y_i=0 \mid x_i ; w)=1-p_i
$$

    将公式写在一起有：

$$
p_i(y_i \mid x_i ; w)=p_i^{y_i}\left(1-p_i\right)^{1-y_i}
$$


+ 参数估计

    设似然函数为：
$$
\prod_{i=1}^{n}p_{i}^{y_{i}}·\left(1-p_{i}\right)^{1-y_{i}}
$$
    对似然函数取负对数有：
$$
L(w)=-\sum_{i=1}^{n}\left(y_{i} \log \left(p_{i}\right)+\left(1-y_{i}\right) \log \left(1-p_{i}\right)\right)
$$
    通过最大化似然函数来对参数进行估计，在这里等价于最小化负对数似然函数 $L(w)$，同样可以得到 $w$ 的估计值。

    下面关于 $L(w)$ 对 $w$ 求导，具体步骤如下：

1. 对于任意样本$x_i$，有：

$$
\begin{aligned}
l &=-y_{i} \log \left(p_{i}\right)-\left(1-y_{i}\right) \log \left(1-p_{i}\right) \\ \\
&=-y_{i} \log \left(p_{i}\right)-\log \left(1-p_{i}\right)+y_{i} \log \left(1-p_{i}\right) \\ \\
&=-y_{i}\left(\log \left(\frac{p_{i}}{1-p_{i}}\right)\right)-\log \left(1-p_{i}\right)
\end{aligned}
$$
2. 由于几率 $\eta_{i}=\frac{p_{i}}{1-p_{i}}$，可得 ${p_i}=\frac{\eta_{i}}{1+\eta_{i}}$，代入：

$$
\begin{aligned}
l &=-y_{i} \log \left(\eta_{i}\right)-\log \left(1-\frac{\eta_{i}}{1+\eta_{i}}\right) \\ \\
&=-y_{i} \log \left(\eta_{i}\right)+\log \left({1+\eta_{i}}\right) \\ \\
&=-y_{i} \log \left(\eta_{i}\right)+\log \left(1+e^{\log \left(\eta_{i}\right)}\right)
\end{aligned}
$$
3. 对几率的对数 $\log(\eta_{i})$ 求导：

$$
\frac{d l}{d \log \left(\eta_{i}\right)}=-y_{i}+\frac{e^{\log \left(\eta_{i}\right)}}{1+e^{\log \left(\eta_{i}\right)}}=-y_{i}+p_{i}
$$

提到过，逻辑回归相当于**对事件的对数几率拟合线性回归**，即：$\log \left(\eta_{i}\right)=\log \frac{p_{i}}{1-p_{i}}=wx_i$，代入有：

$$
\frac{d l}{d \log \left(\eta_{i}\right)} 
=\frac{d l}{d (wx_i)}==-y_{i}+p_{i}
\\
\Downarrow
\\
\frac{d l}{dw}=(-y_{i}+p_{i})x_i
$$
    由于目标是最小化负对数似然函数，所以沿着梯度下降方向：
$$
w \leftarrow w-\frac{\gamma}{n} \sum_{i=1}^{n}\left(-y_{i}+p_{i}\right) x_{i}
\\
其中，\gamma为学习率
$$

## 相似性度量方法
1. **杰卡德（Jaccard）相似系数**
   
   `Jaccard` 系数是衡量**两个集合**的相似度一种指标，计算公式如下：
   $$
   sim_{uv}=\frac{|N(u) \cap N(v)|}{|N(u) \cup N(v)|}
   $$

   + 其中 $N(u)$，$N(v)$ 分别表示用户 $u$ 和用户 $v$ 交互物品的集合。
   
   + 对于用户 $u$ 和 $v$ ，该公式反映了两个交互物品交集的数量占这两个用户交互物品并集的数量的比例。
   
   杰卡德相似系数常用来评估**用户是否会对某物品进行打分**。
   
2. **余弦相似度**
   余弦相似度衡量了两个向量的夹角，夹角越小越相似。余弦相似度的计算如下，其与杰卡德（Jaccard）相似系数只是在分母上存在差异：
   $$
   sim_{uv}=\frac{|N(u) \cap N(v)|}{\sqrt{|N(u)|\cdot|N(v)|}}
   $$
   从向量的角度来看，令矩阵 $A$ 为用户-物品交互矩阵，矩阵的行表示用户，列表示物品。
   
   + 设用户和物品数量分别为 $m,n$，交互矩阵$A$就是一个 $m$ 行 $n$ 列的矩阵。
   
   + 矩阵中的元素均为 $0/1$。若用户 $i$ 对物品 $j$ 存在交互，那么 $A_{i,j}=1$，否则为 $0$ 。

   + 那么，用户之间的相似度为：
     $$
     sim_{uv} = cos(u,v) =\frac{u\cdot v}{|u|\cdot |v|}
     $$
   
     + 向量 $u,v$ 在形式都是 one-hot 类型，$u\cdot v$ 表示向量点积。
   
   上述用户-物品交互矩阵在现实中是十分稀疏的，为了节省内存，交互矩阵会采用**字典**进行存储。在 `sklearn` 中，余弦相似度的实现：

3. **皮尔逊相关系数**

   在用户之间的余弦相似度计算时，**将用户向量的内积展开为各元素乘积和**：
   $$
   sim_{uv} = \frac{\sum_i r_{ui}*r_{vi}}{\sqrt{\sum_i r_{ui}^2}\sqrt{\sum_i r_{vi}^2}}
   $$
   + 其中，$r_{ui},r_{vi}$ 分别表示用户 $u$ 和用户 $v$ 对物品 $i$ 是否有交互(或具体评分值)。
   
   皮尔逊相关系数计算公式：
   $$
   sim(u,v)=\frac{\sum_{i\in I}(r_{ui}-\bar r_u)(r_{vi}-\bar r_v)}{\sqrt{\sum_{i\in I }(r_{ui}-\bar r_u)^2}\sqrt{\sum_{i\in I }(r_{vi}-\bar r_v)^2}}
   $$
   + 其中，$r_{ui},r_{vi}$ 分别表示用户 $u$ 和用户 $v$ 对物品 $i$ 是否有交互(或具体评分值)；
   + $\bar r_u, \bar r_v$ 分别表示用户 $u$ 和用户 $v$ 交互的所有物品交互数量或者评分的平均值；
   
   相较于余弦相似度，皮尔逊相关系数通过使用**用户的平均分**对各独立评分进行修正，减小了用户评分偏置的影响。
   

4. **适用场景**

+ $Jaccard$ 相似度表示两个集合的交集元素个数在并集中所占的比例 ，所以适用于隐式反馈数据（0-1）。
+ 余弦相似度在度量文本相似度、用户相似度、物品相似度的时候都较为常用。
+ 皮尔逊相关度，**实际上也是一种余弦相似度。不过先对向量做了中心化**，范围在 $-1$ 到 $1$。
  + 相关度量的是两个变量的变化趋势是否一致，两个随机变量是不是同增同减。
  + 不适合用作计算布尔值向量（0-1）之间相关度。

## 评价指标

1. 混淆矩阵
   
| **预测结果**     | **实际为正**       | **实际为负**       |
|------------------|--------------------|--------------------|
| **预测为正**     | True Positive (TP)  | False Positive (FP) |
| **预测为负**     | False Negative (FN) | True Negative (TN)  |

2. 召回率

    对用户 $u$ 推荐 $N$ 个物品记为 $R(u)$, 令用户 $u$ 在测试集上喜欢的物品集合为$T(u)$， 那么召回率定义为：

$$ \text{Recall} = \frac{\text{TP}}{\text{TP} + \text{FN}} $$

+ 含义：在模型召回预测的物品中，预测准确的物品占用户实际喜欢的物品的比例。 

3. 精确率
精确率定义为：
$$ \text{Precision} = \frac{\text{TP}}{\text{TP} + \text{FP}} $$ 
+ 含义：推荐的物品中，对用户准确推荐的物品占总物品的比例。 
+ 如要确保召回率高，一般是推荐更多的物品，期望推荐的物品中会涵盖用户喜爱的物品。而实际中，推荐的物品中用户实际喜爱的物品占少数，推荐的精确率就会很低。故同时要确保高召回率和精确率往往是矛盾的，所以实际中需要在二者之间进行权衡。

4. ROC曲线与AUC
   ROC曲线通过计算真正例率(TPR)和假正例率(FPR)值，然后绘制 FPR的TPR图表。ROC 曲线的左上角表示理想模型（TPR接近1，FPR 接近 0）。完全随机的模型对应于对角线（对角线上的任何点表示在某个阈值下随机猜测的模型效果。曲线越靠近左上角，说明模型性能越好。

   * True Positive Rate (TPR)
TPR（recall）表示在所有实际为正的样本中，模型正确预测为正的比例，公式为：

\[
\text{TPR} = \frac{\text{TP}}{\text{TP} + \text{FN}}
\]

   * False Positive Rate (FPR)
FPR 表示在所有实际为负的样本中，模型错误预测为正的比例，公式为：

\[
\text{FPR} = \frac{\text{FP}}{\text{FP} + \text{TN}}
\]

AUC(曲线下面积)，0.5是一个模型预测能力的分界点。小于0.5就是在胡乱猜测。
| AUC = 0.65 | AUC = 0.93 |
|------------|------------|
| ![AUC 0.65](https://developers.google.com/static/machine-learning/crash-course/images/auc_0-65.png?hl=zh-cn) | ![AUC 0.93](https://developers.google.com/static/machine-learning/crash-course/images/auc_0-93.png?hl=zh-cn) |


5. 覆盖率
    覆盖率反映了**推荐算法发掘长尾**的能力，覆盖率越高，说明推荐算法越能将长尾中的物品推荐给用户。
$$
\text { Coverage }=\frac{\left|\bigcup_{u \in U} R(u)\right|}{|I|}
$$

+ 含义：推荐系统能够推荐出来的物品占总物品集合的比例。
  + 其中 $|I|$ 表示所有物品的个数；
  + 系统的用户集合为$U$;
  + 推荐系统给每个用户推荐一个长度为 $N$ 的物品列表$R(u)$.

+ 覆盖率表示最终的推荐列表中包含多大比例的物品。如果所有物品都被给推荐给至少一个用户， 那么覆盖率是100%。

## 卷积输出形状计算公式

假设输入张量的形状为 \([B, C_{\text{in}}, L_{\text{in}}]\)，卷积核的形状为 \([C_{\text{out}}, C_{\text{in}}, K]\)，其中：
- \(B\) 是批次大小（Batch Size）
- \(C_{\text{in}}\) 是输入通道数（Input Channels）
- \(L_{\text{in}}\) 是输入序列长度（Input Length）
- \(C_{\text{out}}\) 是输出通道数（Output Channels）
- \(K\) 是卷积核大小（Kernel Size）

输出形状的计算公式为：
\[
L_{\text{out}} = \left\lfloor \frac{L_{\text{in}} + 2 \cdot \text{padding} - K}{\text{stride}} \right\rfloor + 1
\]

其中，`padding` 是填充大小，`stride` 是步长。

因此，输出张量的形状为：
\[
[B, C_{\text{out}}, L_{\text{out}}]
\]

## Attention 原理
1. 分类
   1. 聚焦式注意力（自上而下）：有预定目的、依赖任务的，主动有意识地聚焦于某一对象的注意力；
   2. 基于显著性的注意力（自下而上）：由外界刺激驱动的注意，不需要主动干预，也和任务无关，自底向上，逐步汇总。eg：最大池化、门控。
2. 计算步骤：
   1. 在所有输入信息上计算注意力分布；
   2. 根据注意力分布来计算输入信息的加权平均．
   具体的，有$X = [x_1, \cdots, x_N] \in \mathbb{R}^{D \times N}$表示N组输入信息，每一组信息含有D维信息表示。

   为了从输入向量中找出与目前特定任务相关的信息，需要构建与任务相关的向量，记作查询向量，然后通过一个打分函数计算input与query之间的相关性。

   当给定一个查询向量$q$时，使用注意力变量 $z \in [1,N]$表示信息的索引位置，也就是$z = n$表示选取第n个输入变量。具体的计算方式如下：
    $$
    \begin{aligned}
    \alpha_n &= p(z = n | X, q) \\
    &= \text{softmax} \left( s(x_n, q) \right) \\
    &= \frac{\exp \left( s(x_n, q) \right)}{\sum_{j=1}^N \exp \left( s(x_j, q) \right)},
    \end{aligned}
    $$
  其中，$\alpha_n$为注意力分布，也就是实际的注意力。因为用来参与矩阵运算的查询向量等是随机生成然后不断更新的，经过上面的式子之后才是真实的注意力权重。$s(x,q)$为注意力打分函数。有以下几种计算方式，其中$W U v$均为可学习参数：
$$
\begin{aligned}
\text{加性模型} &: s(x, q) = v^\top \tanh(Wx + Uq), \\
\text{点积模型} &: s(x, q) = x^\top q, \\
\text{缩放点积模型} &: s(x, q) = \frac{x^\top q}{\sqrt{D}}, \\
\text{双线性模型} &: s(x, q) = x^\top W q,
\end{aligned}
$$
  计算得到特征真实的注意力权重$\alpha_n$时，采取‘软性’信息选择机制对信息进行加权平均。也就是：
$$
\begin{aligned}
\text{att}(X, q) &= \sum_{n=1}^N \alpha_n x_n, \\
&= \mathbb{E}_{z \sim p(z|X, q)} [x_z].
\end{aligned}
$$
3. 变体

**3.1 硬性注意力：**只关注一个变量

* **选取概率最高的向量：** $\text{att}(X, q) = x_{\hat{n}}$, where $\hat{n} = \underset{n=1}{\overset{N}{\arg\max}} \alpha_n$
* **在注意力分布上采取随机采样的方式**

**缺点：** 基于最大采样或随机采样的方式来选择信息，使得最终的损失函数与注意力分布之间的函数关系不可导，无法使用反向传播算法进行训练.

**3.2 键值对注意力：** `key` 用来计算注意力分布，`value` 用来计算聚合信息. 用 $(K, V) = [(k_1, v_1), \dots, (k_N, v_N)]$ 表示 N 组输入信息，当给定任务相关的查询变量 $q$ 时，注意力函数为如下公式. 需要注意的是，当 $K=V$ 时，键值对注意力就等价于普通的注意力机制.

$$
\begin{align}
    \text{att}((K, V), q) &= \sum_{n=1}^N \alpha_n v_n, \tag{8.9} \\
    &= \sum_{n=1}^N \frac{\exp(s(k_n, q))}{\sum_{j=1}^N \exp(s(k_j, q))} v_n, \tag{8.10}
\end{align}
$$

![普通注意力机制与键值对注意力机制对比](md_img\image.png)

左边为普通注意力机制，右边为键值对模式的注意力.

**3.3 多头注意力：**：利用多个注意力查询$Q = [q_1 \ldots q_M]$从输入中选取多组信息，每头注意力关注不同的信息，最后将各自提取的信息concatenate。

## 门控单元GRU

- 重置门与更新门:
  - 作用：重置门用来控制'可能还想记住'的过去状态的数量；更新门允许控制新的状态中有多少旧状态的copy。
  
  ==重置门用来捕获短期依赖关系，更新门用来捕获长期依赖关系==
  
  具体的数学表达，对于给定的时间步 \(t\)，假设输入是一个小批量 \(X_t \in \mathbb{R}^{n \times d}\)（样本个数 \(n\)，输入个数 \(d\)）。上一个时间步的隐状态是 \(H_{t-1} \in \mathbb{R}^{n \times h}\)（隐藏单元个数 \(h\)）。那么，重置门 \(R_t \in \mathbb{R}^{n \times h}\) 和更新门 \(Z_t \in \mathbb{R}^{n \times h}\) 的计算如下所示：

    \[ R_t = \sigma(X_tW_{xr} + H_{t-1}W_{hr} + b_r) \]
    \[ Z_t = \sigma(X_tW_{xz} + H_{t-1}W_{hz} + b_z) \]

    其中 \(W_{xr}, W_{xz} \in \mathbb{R}^{d \times h}\) 和 \(W_{hr}, W_{hz} \in \mathbb{R}^{h \times h}\) 是权重参数，\(b_r, b_z \in \mathbb{R}^{1 \times h}\) 是偏置参数。我们使用 sigmoid 函数将输入值转换到区间 \((0,1)\)。
    具体如下图

<div style="text-align: center;">
<img src="md_img\image2.png" alt="图片说明" style="max-width: 100%; height: auto;">
</div>

- 候选隐状态：
  - 公式：时间步t的候选隐状态\(\hat{H_t} \in  \mathbb{R}^{n \times h}\)，计算如下：
  \(\hat{H_t} = tanh(X_tW_{Xh} + (R_t \odot H_{t-1})W_{hh} + b_h)\)
  \(R_t\) 与 \(H_{t-1}\)的元素相乘能够减少以往状态的影响。当重置门中的项接近1，上式中\(R_t\)与\(H_{t-1}\)的哈达玛积变为只有\(H_{t-1}\)，也就变成了一个普通的循环神经网络；对重置门中所有接近0的项，候选隐状态只剩下\(X_t\)。所以，所有预先存在的隐状态都会被重置为默认值。
<div style="text-align: center;">
<img src="md_img\image3.png" alt="图片说明" style="max-width: 100%; height: auto;">
</div>
    
- 隐状态
    在得到候选隐状态后，还需要结合更新门\(Z_t\)的效果。这是为了确定新的隐状态在多大程度上来自旧状态和新的候选状态。只需要进行凸组合就可以实现。所以门控循环单元的整体更新公式为：
    $$
    \mathbf{H}_t = \mathbf{Z}t \odot \mathbf{H}{t-1} + (1 - \mathbf{Z}_t) \odot \tilde{\mathbf{H}}_t
    $$
    由上面的式子，可以很显然的看出，当\(Z_t\)接近1时，模型倾向于保存旧状态，也就是来自当前时间步的X的信息基本上被忽略不计；反之，如果接近于0，则新的隐状态就接近候选隐状态。这能够处理循环神经网络中的梯度消失问题，并能够更好地捕获时间步距离很长的序列的依赖关系。
    具体结构如下：
<div style="text-align: center;">
<img src="md_img\image4.png" alt="图片说明" style="max-width: 100%; height: auto;">
</div>