# 基于协同过滤的召回
## Use-CF & Item-CF

**示例图片**
![image-20210629232622758](http://ryluo.oss-cn-chengdu.aliyuncs.com/图片image-20210629232622758.png)

### user

**计算过程**
1. 计算用户之间的相似度

   + 根据几种评价指标,计算出各用户之间的相似程度。对于用户 Alice，选取出与其最相近的 $N$ 个用户。

2. 计算用户对新物品的评分预测

   + 方法一：利用目标用户与相似用户之间的相似度以及相似用户对物品的评分，来预测目标用户对候选物品的评分估计：
     $$
     R_{\mathrm{u}, \mathrm{p}}=\frac{\sum_{\mathrm{s} \in S}\left(w_{\mathrm{u}, \mathrm{s}} \cdot R_{\mathrm{s}, \mathrm{p}}\right)}{\sum_{\mathrm{s} \in S} w_{\mathrm{u}, \mathrm{s}}}
     $$

     + 其中，权重 $w_{u,s}$ 是用户 $u$ 和用户 $s$ 的相似度， $R_{s,p}$ 是用户 $s$ 对物品 $p$ 的评分。

   + 方法二：考虑到用户评分的偏置，即有的用户喜欢打高分， 有的用户喜欢打低分的情况。公式如下：
     $$
     R_{\mathrm{u}, \mathrm{p}}=\bar{R}_{u} + \frac{\sum_{\mathrm{s} \in S}\left(w_{\mathrm{u}, \mathrm{s}} \cdot \left(R_{s, p}-\bar{R}_{s}\right)\right)}{\sum_{\mathrm{s} \in S} w_{\mathrm{u}, \mathrm{s}}}
     $$

     + 其中，$\bar{R}_{s}$ 表示用户 $s$ 对物品的历史平均评分。

3. 对用户进行物品推荐

   + 在获得用户 $u$ 对不同物品的评价预测后， 最终的推荐列表根据预测评分进行排序得到。 

### item

计算过程
1. 计算物品之间的相似度：余弦相似性或者皮尔逊相关系数。选取TopN相似物品计算评分
2. 根据选取出来的TopN，与user的计算方式相同，计算得分。

由于物品推荐存在长尾效应，需要对推荐权重进行控制，以防止推荐内容过于同质化。

* base 公式
  $$
  w_{i j}=\frac{|N(i) \bigcap N(j)|}{|N(i)|}
  $$

  + 该公式表示同时喜好物品 $i$ 和物品 $j$ 的用户数，占喜爱物品 $i$ 的比例。
  + 缺点：若物品 $j$ 为热门物品，那么它与任何物品的相似度都很高。

* 控制对热门物品的惩罚力度
  $$
  w_{i j}=\frac{|N(i) \cap N(j)|}{|N(i)|^{1-\alpha}|N(j)|^{\alpha}}
  $$

  * 参数 $\alpha$为可控惩罚力度。

* 对活跃用户的惩罚

  * 在计算物品之间的相似度时，可以进一步将用户的活跃度考虑进来。
    $$
    w_{i j}=\frac{\sum_{\operatorname{\text {u}\in N(i) \cap N(j)}} \frac{1}{\log 1+|N(u)|}}{|N(i)|^{1-\alpha}|N(j)|^{\alpha}}
    $$

### 弊端
1. 泛化能力弱，无法将两个物品的相似信息推广到其他物品上，进而导致热门物品具有**很强的头部效应**，容易跟大量物品产生相似，而尾部物品由于特征向量稀疏，导致很少被推荐，也就是推荐时的同质化。
2. 需要考虑用户评分倾向，一部分用户倾向高分，一部分用户倾向低分。需要对相似度计算进行改进，否则会造成错误分类
3. 没有利用更多的用户或者物品信息，只使用交互信息进行计算，整体模型的表达能力十分有限。

## 矩阵分解

+ 原理图：
<img src="https://img-blog.csdnimg.cn/20200822212051499.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3d1emhvbmdxaWFuZw==,size_1,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述" style="zoom:80%;" />

+ 基本思想
  - 用户矩阵Q
    表示不同用户**对于不同元素的偏好程度**，取值范围[0,1]，1代表很喜欢，0代表不喜欢。
  - 物品矩阵P
    表示**每种物品含有各种元素（特征）的成分**
  - 计算：
    通过将对应向量的元素做内积运算，得到用户对该物品的喜好程度。相应的可以得到用户对每个物品的评分矩阵

+ FunkSVD with Bias 算法
  - 思想：**把求解P、Q的参数问题转换成最优化问题，通过训练集里面的观察值,利用最小化来学习用户矩阵和物品矩阵。**
  - 算法过程
  1. 在有用户矩阵和物品矩阵的前提下，若要计算用户 $u$ 对物品 $i$ 的评分， 可以根据公式：
   $$
   \operatorname{Preference}(u, i)=r_{u i}=p_{u}^{T} q_{i}=\sum_{k=1}^{K} p_{u, k} q_{i,k}
   $$
   其中，向量 $p_u$ 表示用户 $u$ 的隐向量，向量  $q_i$ 表示物品 $i$ 的隐向量。

  2. **随机初始化**一个用户矩阵 $U$ 和一个物品矩阵 $V$，获取每个用户和物品的初始隐语义向量。一般是与`1/sqrt(F)`成正比，F为隐向量维度。

  3. 计算预测得分 with bias
   $$\hat{r}_{u i}=\mu+b_{u}+b_{i}+p_{u}^{T} \cdot q_{i}$$.其中，$\mu$反映推荐模型整体的平均评分，一般为样本均值；$b_u$为用户偏差系数。可以使用用户 $u$ 的评分均值，也可作为训练参数；$b_i$为物品偏差系数。可以使用物品 $i$ 的得分均值，也可以当做训练参数。

  4. 对于评分矩阵中的每个元素，计算预测误差 $e_{u i}=r_{u i}-\hat{r}_{u i}$，对所有训练样本的平方误差进行累加：
   $$
   \operatorname{SSE}=\sum_{u, i} e_{u i}^{2}=\sum_{u, i}\left(r_{u i}-\sum_{k=1}^{K} p_{u,k} q_{i,k}\right)^{2}
   $$为方便后续求解，给 $SSE$ 增加系数 $1/2$ ：
     $$
     \operatorname{SSE}=\frac{1}{2} \sum_{u, i} e_{u i}^{2}=\frac{1}{2} \sum_{u, i}\left(r_{u i}-\sum_{k=1}^{K} p_{u k} q_{i k}\right)^{2}
     $$

  5.  前面提到，模型预测越准确等价于预测误差越小，那么优化的目标函数变为：
  $$
  \begin{aligned}
  \min _{q^{*}, p^{*}} \frac{1}{2} \sum_{(u, i) \in K} &\left(r_{u i}-\left(\mu+b_{u}+b_{i}+q_{i}^{T} p_{u}\right)\right)^{2} \\
  &+\lambda\left(\left\|p_{u}\right\|^{2}+\left\|q_{i}\right\|^{2}+b_{u}^{2}+b_{i}^{2}\right)
  \end{aligned}
  $$ 
  $K$ 表示所有用户评分样本的集合，**即评分矩阵中不为空的元素**，其他空缺值在测试时是要预测的。该目标函数需要优化的目标是用户矩阵 $U$ 和一个物品矩阵 $V$。

  1. 对于给定的目标函数，可以通过梯度下降法对参数进行优化。

   + 求解目标函数 $SSE$ 关于用户矩阵中参数 $p_{u,k}$ 的梯度：
     $$
     \frac{\partial}{\partial p_{u,k}} S S E=\frac{\partial}{\partial p_{u,k}}\left(\frac{1}{2}e_{u i}^{2}\right) =e_{u i} \frac{\partial}{\partial p_{u,k}} e_{u i}=e_{u i} \frac{\partial}{\partial p_{u,k}}\left(r_{u i}-\sum_{k=1}^{K} p_{u,k} q_{i,k}\right)=-e_{u i} q_{i,k}
     $$

   + 求解目标函数 $SSE$ 关于 $q_{i,k}$ 的梯度：
     $$
     \frac{\partial}{\partial q_{i,k}} S S E=\frac{\partial}{\partial q_{i,k}}\left(\frac{1}{2}e_{u i}^{2}\right) =e_{u i} \frac{\partial}{\partial q_{i,k}} e_{u i}=e_{u i} \frac{\partial}{\partial q_{i,k}}\left(r_{u i}-\sum_{k=1}^{K} p_{u,k} q_{i,k}\right)=-e_{u i} p_{u,k}
     $$

  2. 参数梯度更新
   $$
   p_{u, k}=p_{u,k}-\eta (-e_{ui}q_{i, k})=p_{u,k}+\eta e_{ui}q_{i, k} \\ 
   q_{i, k}=q_{i,k}-\eta (-e_{ui}p_{u,k})=q_{i, k}+\eta e_{ui}p_{u, k}\\
   \frac{\partial}{\partial b_{i}} S S E=-e_{u i}+\lambda b_{i}\\
   \frac{\partial}{\partial b_{u}} S S E=-e_{u i} +\lambda b_{u}
   $$ 其中，$\eta$ 表示学习率，用于控制步长。当**参数很多**的时候，就是两个矩阵很大的时候，往往容易陷入**过拟合**的困境，需要在目标函数上面加上正则化的损失。

  - 原理图，==分解出的k维特征是模型待学习的参数==
<div align=center>
<img src="https://img-blog.csdnimg.cn/20200823101513233.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3d1emhvbmdxaWFuZw==,size_1,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述" style="zoom:70%;" />
</div>

## Neural Collaborative Filtering (NeuralCF)

### 模型结构
<div align="center">
  <img src="https://static001.geekbang.org/resource/image/5f/2c/5ff301f11e686eedbacd69dee184312c.jpg" alt="NeuralCF Model" style="zoom:33%;">
</div>

### 原理
- **输入特征**：用户ID和项目ID转换为OneHot向量，并映射成指定维度的稠密向量，解决冷启动问题。
- **GMF（广义矩阵分解部分）**：用户与物品特征矩阵逐元素点积，通过线性方法学习特征。
- **MLP**：设置用户和物品的MLP，通过特征组合拼接embedding，经过线性层得到MLP的输出。
- **最终输出**：拼接GMF与MLP特征，通过线性层得到最终输出。

### 优化
$$
L_{sqr}=\sum_{(u,i)\in y\cup y^-}w_{ui}(y_{ui}-\hat{y}_{ui})^2
$$
其中 $w_{ui}$ 为超参数，给每个样本赋权重。

$$
p(y,y^-|P,Q,\Theta_f)=\prod_{(u,i)\in{y}}\hat{y}_{ui}\prod_{(u,j)\in{y^-}}(1-\hat{y}_{uj})
$$

$$
L=-\sum_{(u,i)\in{y}}log\hat{y}_{ui}-\sum_{(u,j)\in{y^-}}log(1-\hat{y}_{uj})=-\sum_{(u,i)\in{y}\cup{y}^-}y_{ui}log \hat{y}_{ui}+(1-y_{ui})log(1-\hat{y}_{ui})
$$
使用随机梯度下降（SGD）进行训练优化。这个函数等价于交叉熵损失函数(binary cross-entropy loss)。通过对NCF进行概率处理，将隐性反馈的推荐问题当做二分类问题来解决。对于负样本 $y^-$，每次迭代均匀地从未观察到交互作用中采样作为负样本，控制采样比例。

**注**：MF是NCF模型的一个特例。

# 基于向量召回
## FM模型（分解机）

### 模型形式

$$
y = w_0+\sum_{i=1}^nw_ix_i+\sum_{i=1}^{n}\sum_{i+1}^n\lt v_i,v_j\gt x_ix_j
$$

改进思想：**为每个 $x_i$ 计算一个 embedding，然后将两个向量之间的 embedding 做内积得到之前所谓的 $w_{ij}$**。这使得即使两个特征之前从未在训练集中**同时**出现，只需要 $x_i$ 和其他的 $x_k$ 同时出现过就可以计算出 $x_i$ 的 embedding，大大提升了模型的泛化能力。**当交叉项参数 $w_{ij}$ 全为 0 的时候，整个模型就退化为普通的 LR 模型**。

---

公式中各参数的意义：

- $\omega_{0}$：**全局偏置**；
- $\omega_{i}$：模型第 $i$ 个变量的权重；
- $\omega_{ij} = < v_{i}, v_{j}>$：特征 $i$ 和 $j$ 的交叉权重；
- $v_{i}$：第 $i$ 维特征的隐向量；
- $<\cdot, \cdot>$：向量点积；
- $k(k<<n)$：隐向量的长度，包含 $k$ 个描述特征的因子。

---

* 降低时间复杂度的证明

利用 $\sum$ 运算的线性性质，公式可以化简为：

$$
\begin{align} 
\sum_{i=1}^{n-1}{\sum_{j=i+1}^{n}{<v_i,v_j>x_ix_j}}
&= \frac{1}{2}\sum_{i=1}^{n}{\sum_{j=1}^{n}{<v_i,v_j>x_ix_j}} - \frac{1}{2} {\sum_{i=1}^{n}{<v_i,v_i>x_ix_i}} \\
&= \frac{1}{2} \left( \sum_{i=1}^{n}{\sum_{j=1}^{n}{\sum_{f=1}^{k}{v_{i,f}v_{j,f}x_ix_j}}} - \sum_{i=1}^{n}{\sum_{f=1}^{k}{v_{i,f}v_{i,f}x_ix_i}} \right) \\
&= \frac{1}{2}\sum_{f=1}^{k}{\left[ \left( \sum_{i=1}^{n}{v_{i,f}x_i} \right) \cdot \left( \sum_{j=1}^{n}{v_{j,f}x_j} \right) - \sum_{i=1}^{n}{v_{i,f}^2 x_i^2} \right]} \\
&= \frac{1}{2}\sum_{f=1}^{k}{\left[ \left( \sum_{i=1}^{n}{v_{i,f}x_i} \right)^2 - \sum_{i=1}^{n}{v_{i,f}^2 x_i^2} \right]} 
\end{align}
$$

---

* 解释

- $v_{i,f}$ 是一个具体的值；
- **第 1 个等号**：对称矩阵 $W$ 的对角线上半部分；
- **第 2 个等号**：将向量内积 $<v_i, v_j>$ 展开为累加和的形式；
- **第 3 个等号**：提出公共部分；
- **第 4 个等号**：$i$ 和 $j$ 相当于表示为相同的平方过程。

### 用于召回的改进