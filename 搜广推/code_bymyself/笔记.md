# 机器学习模型
## Logistic回归

+ sigmoid公式：
$$
f(x)=\frac{1}{1+e^{-x}}
$$

+ sigmoid函数图像

<img src="http://ryluo.oss-cn-chengdu.aliyuncs.com/图片Sigmoid_function.png" alt="Sigmoid function" style="display: block; margin: 0 auto;">

+ 导数
\[
\sigma'(x) = \sigma(x) \cdot (1 - \sigma(x))
\]

+ LR分类器 ==二分类==
+ 表达式
$$
p_i(y_i=1 \mid x_i) = \frac{1}{1+e^{-w x_i}} \\ \\
p_i(y_i=0 \mid x_i) = \frac{e^{-w x_i}}{1+e^{-w x_i}}
$$
    其中，随机变量$x_i \in \mathbb{R}^{n}$为实数，随机变量$y_i$的取值为$\{0, 1\}$，参数$w\in \mathbb{R}^{n}$。$wx_i$表示变量$x_i$与参数$w$之间的内积。

+ 几率 ： ==事件发生概率与不发生的概率的比值
$$
\eta_{i}=\frac{p_{i}}{1-p_{i}}
$$

    将等式两边化为以`e`为底的指数函数，有：
$$
e^{wx_i} =\frac{p_{i}}{1-p_{i}}
\\ \Downarrow
\\
p_i = \frac{1}{1+e^{-w x_i}}
$$
    逻辑回归本质上就是**关于事件几率的线性回归**。当然，逻辑回归和线性回归存在本质上的不同，其次在损失函数上，线性回归中的损失函数是均方误差，而 Logistic 回归的损失函数是**负对数似然（Negative Log-Likelihood）**。

+ 假设

    逻辑回归模型的基本假设是$y_i$服从伯努利分布，也称为两点分布或者$0-1$分布。即：
$$
p_i(y_i=1 \mid x_i ; w)=p_i \\
p_i(y_i=0 \mid x_i ; w)=1-p_i
$$

    将公式写在一起有：

$$
p_i(y_i \mid x_i ; w)=p_i^{y_i}\left(1-p_i\right)^{1-y_i}
$$


+ 参数估计

    设似然函数为：
$$
\prod_{i=1}^{n}p_{i}^{y_{i}}·\left(1-p_{i}\right)^{1-y_{i}}
$$
    对似然函数取负对数有：
$$
L(w)=-\sum_{i=1}^{n}\left(y_{i} \log \left(p_{i}\right)+\left(1-y_{i}\right) \log \left(1-p_{i}\right)\right)
$$
    通过最大化似然函数来对参数进行估计，在这里等价于最小化负对数似然函数 $L(w)$，同样可以得到 $w$ 的估计值。

    下面关于 $L(w)$ 对 $w$ 求导，具体步骤如下：

1. 对于任意样本$x_i$，有：

$$
\begin{aligned}
l &=-y_{i} \log \left(p_{i}\right)-\left(1-y_{i}\right) \log \left(1-p_{i}\right) \\ \\
&=-y_{i} \log \left(p_{i}\right)-\log \left(1-p_{i}\right)+y_{i} \log \left(1-p_{i}\right) \\ \\
&=-y_{i}\left(\log \left(\frac{p_{i}}{1-p_{i}}\right)\right)-\log \left(1-p_{i}\right)
\end{aligned}
$$
2. 由于几率 $\eta_{i}=\frac{p_{i}}{1-p_{i}}$，可得 ${p_i}=\frac{\eta_{i}}{1+\eta_{i}}$，代入：

$$
\begin{aligned}
l &=-y_{i} \log \left(\eta_{i}\right)-\log \left(1-\frac{\eta_{i}}{1+\eta_{i}}\right) \\ \\
&=-y_{i} \log \left(\eta_{i}\right)+\log \left({1+\eta_{i}}\right) \\ \\
&=-y_{i} \log \left(\eta_{i}\right)+\log \left(1+e^{\log \left(\eta_{i}\right)}\right)
\end{aligned}
$$
3. 对几率的对数 $\log(\eta_{i})$ 求导：

$$
\frac{d l}{d \log \left(\eta_{i}\right)}=-y_{i}+\frac{e^{\log \left(\eta_{i}\right)}}{1+e^{\log \left(\eta_{i}\right)}}=-y_{i}+p_{i}
$$

提到过，逻辑回归相当于**对事件的对数几率拟合线性回归**，即：$\log \left(\eta_{i}\right)=\log \frac{p_{i}}{1-p_{i}}=wx_i$，代入有：

$$
\frac{d l}{d \log \left(\eta_{i}\right)} 
=\frac{d l}{d (wx_i)}==-y_{i}+p_{i}
\\
\Downarrow
\\
\frac{d l}{dw}=(-y_{i}+p_{i})x_i
$$
    由于目标是最小化负对数似然函数，所以沿着梯度下降方向：
$$
w \leftarrow w-\frac{\gamma}{n} \sum_{i=1}^{n}\left(-y_{i}+p_{i}\right) x_{i}
\\
其中，\gamma为学习率
$$

## 相似性度量方法
1. **杰卡德（Jaccard）相似系数**
   
   `Jaccard` 系数是衡量**两个集合**的相似度一种指标，计算公式如下：
   $$
   sim_{uv}=\frac{|N(u) \cap N(v)|}{|N(u) \cup N(v)|}
   $$

   + 其中 $N(u)$，$N(v)$ 分别表示用户 $u$ 和用户 $v$ 交互物品的集合。
   
   + 对于用户 $u$ 和 $v$ ，该公式反映了两个交互物品交集的数量占这两个用户交互物品并集的数量的比例。
   
   杰卡德相似系数常用来评估**用户是否会对某物品进行打分**。
   
2. **余弦相似度**
   余弦相似度衡量了两个向量的夹角，夹角越小越相似。余弦相似度的计算如下，其与杰卡德（Jaccard）相似系数只是在分母上存在差异：
   $$
   sim_{uv}=\frac{|N(u) \cap N(v)|}{\sqrt{|N(u)|\cdot|N(v)|}}
   $$
   从向量的角度来看，令矩阵 $A$ 为用户-物品交互矩阵，矩阵的行表示用户，列表示物品。
   
   + 设用户和物品数量分别为 $m,n$，交互矩阵$A$就是一个 $m$ 行 $n$ 列的矩阵。
   
   + 矩阵中的元素均为 $0/1$。若用户 $i$ 对物品 $j$ 存在交互，那么 $A_{i,j}=1$，否则为 $0$ 。

   + 那么，用户之间的相似度为：
     $$
     sim_{uv} = cos(u,v) =\frac{u\cdot v}{|u|\cdot |v|}
     $$
   
     + 向量 $u,v$ 在形式都是 one-hot 类型，$u\cdot v$ 表示向量点积。
   
   上述用户-物品交互矩阵在现实中是十分稀疏的，为了节省内存，交互矩阵会采用**字典**进行存储。在 `sklearn` 中，余弦相似度的实现：

3. **皮尔逊相关系数**

   在用户之间的余弦相似度计算时，**将用户向量的内积展开为各元素乘积和**：
   $$
   sim_{uv} = \frac{\sum_i r_{ui}*r_{vi}}{\sqrt{\sum_i r_{ui}^2}\sqrt{\sum_i r_{vi}^2}}
   $$
   + 其中，$r_{ui},r_{vi}$ 分别表示用户 $u$ 和用户 $v$ 对物品 $i$ 是否有交互(或具体评分值)。
   
   皮尔逊相关系数计算公式：
   $$
   sim(u,v)=\frac{\sum_{i\in I}(r_{ui}-\bar r_u)(r_{vi}-\bar r_v)}{\sqrt{\sum_{i\in I }(r_{ui}-\bar r_u)^2}\sqrt{\sum_{i\in I }(r_{vi}-\bar r_v)^2}}
   $$
   + 其中，$r_{ui},r_{vi}$ 分别表示用户 $u$ 和用户 $v$ 对物品 $i$ 是否有交互(或具体评分值)；
   + $\bar r_u, \bar r_v$ 分别表示用户 $u$ 和用户 $v$ 交互的所有物品交互数量或者评分的平均值；
   
   相较于余弦相似度，皮尔逊相关系数通过使用**用户的平均分**对各独立评分进行修正，减小了用户评分偏置的影响。
   

4. **适用场景**

+ $Jaccard$ 相似度表示两个集合的交集元素个数在并集中所占的比例 ，所以适用于隐式反馈数据（0-1）。
+ 余弦相似度在度量文本相似度、用户相似度、物品相似度的时候都较为常用。
+ 皮尔逊相关度，**实际上也是一种余弦相似度。不过先对向量做了中心化**，范围在 $-1$ 到 $1$。
  + 相关度量的是两个变量的变化趋势是否一致，两个随机变量是不是同增同减。
  + 不适合用作计算布尔值向量（0-1）之间相关度。


## Use-CF & Item-CF

**示例图片**
![image-20210629232622758](http://ryluo.oss-cn-chengdu.aliyuncs.com/图片image-20210629232622758.png)

### user

**计算过程**
1. 计算用户之间的相似度

   + 根据几种评价指标,计算出各用户之间的相似程度。对于用户 Alice，选取出与其最相近的 $N$ 个用户。

2. 计算用户对新物品的评分预测

   + 方法一：利用目标用户与相似用户之间的相似度以及相似用户对物品的评分，来预测目标用户对候选物品的评分估计：
     $$
     R_{\mathrm{u}, \mathrm{p}}=\frac{\sum_{\mathrm{s} \in S}\left(w_{\mathrm{u}, \mathrm{s}} \cdot R_{\mathrm{s}, \mathrm{p}}\right)}{\sum_{\mathrm{s} \in S} w_{\mathrm{u}, \mathrm{s}}}
     $$

     + 其中，权重 $w_{u,s}$ 是用户 $u$ 和用户 $s$ 的相似度， $R_{s,p}$ 是用户 $s$ 对物品 $p$ 的评分。

   + 方法二：考虑到用户评分的偏置，即有的用户喜欢打高分， 有的用户喜欢打低分的情况。公式如下：
     $$
     R_{\mathrm{u}, \mathrm{p}}=\bar{R}_{u} + \frac{\sum_{\mathrm{s} \in S}\left(w_{\mathrm{u}, \mathrm{s}} \cdot \left(R_{s, p}-\bar{R}_{s}\right)\right)}{\sum_{\mathrm{s} \in S} w_{\mathrm{u}, \mathrm{s}}}
     $$

     + 其中，$\bar{R}_{s}$ 表示用户 $s$ 对物品的历史平均评分。

3. 对用户进行物品推荐

   + 在获得用户 $u$ 对不同物品的评价预测后， 最终的推荐列表根据预测评分进行排序得到。 

### item

计算过程
1. 计算物品之间的相似度：余弦相似性或者皮尔逊相关系数。选取TopN相似物品计算评分
2. 根据选取出来的TopN，与user的计算方式相同，计算得分。

由于物品推荐存在长尾效应，需要对推荐权重进行控制，以防止推荐内容过于同质化。

* base 公式
  $$
  w_{i j}=\frac{|N(i) \bigcap N(j)|}{|N(i)|}
  $$

  + 该公式表示同时喜好物品 $i$ 和物品 $j$ 的用户数，占喜爱物品 $i$ 的比例。
  + 缺点：若物品 $j$ 为热门物品，那么它与任何物品的相似度都很高。

* 控制对热门物品的惩罚力度
  $$
  w_{i j}=\frac{|N(i) \cap N(j)|}{|N(i)|^{1-\alpha}|N(j)|^{\alpha}}
  $$

  * 参数 $\alpha$为可控惩罚力度。

* 对活跃用户的惩罚

  * 在计算物品之间的相似度时，可以进一步将用户的活跃度考虑进来。
    $$
    w_{i j}=\frac{\sum_{\operatorname{\text {u}\in N(i) \cap N(j)}} \frac{1}{\log 1+|N(u)|}}{|N(i)|^{1-\alpha}|N(j)|^{\alpha}}
    $$

### 评价指标

1. 混淆矩阵
   
| **预测结果**     | **实际为正**       | **实际为负**       |
|------------------|--------------------|--------------------|
| **预测为正**     | True Positive (TP)  | False Positive (FP) |
| **预测为负**     | False Negative (FN) | True Negative (TN)  |

2. 召回率

    对用户 $u$ 推荐 $N$ 个物品记为 $R(u)$, 令用户 $u$ 在测试集上喜欢的物品集合为$T(u)$， 那么召回率定义为：

$$ \text{Recall} = \frac{\text{TP}}{\text{TP} + \text{FN}} $$

+ 含义：在模型召回预测的物品中，预测准确的物品占用户实际喜欢的物品的比例。 

3. 精确率
精确率定义为：
$$ \text{Precision} = \frac{\text{TP}}{\text{TP} + \text{FP}} $$ 
+ 含义：推荐的物品中，对用户准确推荐的物品占总物品的比例。 
+ 如要确保召回率高，一般是推荐更多的物品，期望推荐的物品中会涵盖用户喜爱的物品。而实际中，推荐的物品中用户实际喜爱的物品占少数，推荐的精确率就会很低。故同时要确保高召回率和精确率往往是矛盾的，所以实际中需要在二者之间进行权衡。

4. ROC曲线与AUC
   ROC曲线通过计算真正例率(TPR)和假正例率(FPR)值，然后绘制 FPR的TPR图表。ROC 曲线的左上角表示理想模型（TPR接近1，FPR 接近 0）。完全随机的模型对应于对角线（对角线上的任何点表示在某个阈值下随机猜测的模型效果。曲线越靠近左上角，说明模型性能越好。

   * True Positive Rate (TPR)
TPR（recall）表示在所有实际为正的样本中，模型正确预测为正的比例，公式为：

\[
\text{TPR} = \frac{\text{TP}}{\text{TP} + \text{FN}}
\]

   * False Positive Rate (FPR)
FPR 表示在所有实际为负的样本中，模型错误预测为正的比例，公式为：

\[
\text{FPR} = \frac{\text{FP}}{\text{FP} + \text{TN}}
\]

AUC(曲线下面积)，0.5是一个模型预测能力的分界点。小于0.5就是在胡乱猜测。
| AUC = 0.65 | AUC = 0.93 |
|------------|------------|
| ![AUC 0.65](https://developers.google.com/static/machine-learning/crash-course/images/auc_0-65.png?hl=zh-cn) | ![AUC 0.93](https://developers.google.com/static/machine-learning/crash-course/images/auc_0-93.png?hl=zh-cn) |


5. 覆盖率
    覆盖率反映了**推荐算法发掘长尾**的能力，覆盖率越高，说明推荐算法越能将长尾中的物品推荐给用户。
$$
\text { Coverage }=\frac{\left|\bigcup_{u \in U} R(u)\right|}{|I|}
$$

+ 含义：推荐系统能够推荐出来的物品占总物品集合的比例。
  + 其中 $|I|$ 表示所有物品的个数；
  + 系统的用户集合为$U$;
  + 推荐系统给每个用户推荐一个长度为 $N$ 的物品列表$R(u)$.

+ 覆盖率表示最终的推荐列表中包含多大比例的物品。如果所有物品都被给推荐给至少一个用户， 那么覆盖率是100%。

### 弊端
1. 泛化能力弱，无法将两个物品的相似信息推广到其他物品上，进而导致热门物品具有**很强的头部效应**，容易跟大量物品产生相似，而尾部物品由于特征向量稀疏，导致很少被推荐，也就是推荐时的同质化。
2. 需要考虑用户评分倾向，一部分用户倾向高分，一部分用户倾向低分。需要对相似度计算进行改进，否则会造成错误分类
3. 没有利用更多的用户或者物品信息，只使用交互信息进行计算，整体模型的表达能力十分有限。

## 矩阵分解

+ 原理图：
<img src="https://img-blog.csdnimg.cn/20200822212051499.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3d1emhvbmdxaWFuZw==,size_1,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述" style="zoom:80%;" />

+ 基本思想
  - 用户矩阵Q
    表示不同用户**对于不同元素的偏好程度**，取值范围[0,1]，1代表很喜欢，0代表不喜欢。
  - 物品矩阵P
    表示**每种物品含有各种元素（特征）的成分**
  - 计算：
    通过将对应向量的元素做内积运算，得到用户对该物品的喜好程度。相应的可以得到用户对每个物品的评分矩阵

+ FunkSVD with Bias 算法
  - 思想：**把求解P、Q的参数问题转换成最优化问题，通过训练集里面的观察值,利用最小化来学习用户矩阵和物品矩阵。**
  - 算法过程
  1. 在有用户矩阵和物品矩阵的前提下，若要计算用户 $u$ 对物品 $i$ 的评分， 可以根据公式：
   $$
   \operatorname{Preference}(u, i)=r_{u i}=p_{u}^{T} q_{i}=\sum_{k=1}^{K} p_{u, k} q_{i,k}
   $$
   其中，向量 $p_u$ 表示用户 $u$ 的隐向量，向量  $q_i$ 表示物品 $i$ 的隐向量。

  2. **随机初始化**一个用户矩阵 $U$ 和一个物品矩阵 $V$，获取每个用户和物品的初始隐语义向量。一般是与`1/sqrt(F)`成正比，F为隐向量维度。

  3. 计算预测得分 with bias
   $$\hat{r}_{u i}=\mu+b_{u}+b_{i}+p_{u}^{T} \cdot q_{i}$$.其中，$\mu$反映推荐模型整体的平均评分，一般为样本均值；$b_u$为用户偏差系数。可以使用用户 $u$ 的评分均值，也可作为训练参数；$b_i$为物品偏差系数。可以使用物品 $i$ 的得分均值，也可以当做训练参数。

  4. 对于评分矩阵中的每个元素，计算预测误差 $e_{u i}=r_{u i}-\hat{r}_{u i}$，对所有训练样本的平方误差进行累加：
   $$
   \operatorname{SSE}=\sum_{u, i} e_{u i}^{2}=\sum_{u, i}\left(r_{u i}-\sum_{k=1}^{K} p_{u,k} q_{i,k}\right)^{2}
   $$为方便后续求解，给 $SSE$ 增加系数 $1/2$ ：
     $$
     \operatorname{SSE}=\frac{1}{2} \sum_{u, i} e_{u i}^{2}=\frac{1}{2} \sum_{u, i}\left(r_{u i}-\sum_{k=1}^{K} p_{u k} q_{i k}\right)^{2}
     $$

  5.  前面提到，模型预测越准确等价于预测误差越小，那么优化的目标函数变为：
  $$
  \begin{aligned}
  \min _{q^{*}, p^{*}} \frac{1}{2} \sum_{(u, i) \in K} &\left(r_{u i}-\left(\mu+b_{u}+b_{i}+q_{i}^{T} p_{u}\right)\right)^{2} \\
  &+\lambda\left(\left\|p_{u}\right\|^{2}+\left\|q_{i}\right\|^{2}+b_{u}^{2}+b_{i}^{2}\right)
  \end{aligned}
  $$ $K$ 表示所有用户评分样本的集合，**即评分矩阵中不为空的元素**，其他空缺值在测试时是要预测的。该目标函数需要优化的目标是用户矩阵 $U$ 和一个物品矩阵 $V$。

  6. 对于给定的目标函数，可以通过梯度下降法对参数进行优化。

   + 求解目标函数 $SSE$ 关于用户矩阵中参数 $p_{u,k}$ 的梯度：
     $$
     \frac{\partial}{\partial p_{u,k}} S S E=\frac{\partial}{\partial p_{u,k}}\left(\frac{1}{2}e_{u i}^{2}\right) =e_{u i} \frac{\partial}{\partial p_{u,k}} e_{u i}=e_{u i} \frac{\partial}{\partial p_{u,k}}\left(r_{u i}-\sum_{k=1}^{K} p_{u,k} q_{i,k}\right)=-e_{u i} q_{i,k}
     $$

   + 求解目标函数 $SSE$ 关于 $q_{i,k}$ 的梯度：
     $$
     \frac{\partial}{\partial q_{i,k}} S S E=\frac{\partial}{\partial q_{i,k}}\left(\frac{1}{2}e_{u i}^{2}\right) =e_{u i} \frac{\partial}{\partial q_{i,k}} e_{u i}=e_{u i} \frac{\partial}{\partial q_{i,k}}\left(r_{u i}-\sum_{k=1}^{K} p_{u,k} q_{i,k}\right)=-e_{u i} p_{u,k}
     $$

  7. 参数梯度更新
   $$
   p_{u, k}=p_{u,k}-\eta (-e_{ui}q_{i, k})=p_{u,k}+\eta e_{ui}q_{i, k} \\ 
   q_{i, k}=q_{i,k}-\eta (-e_{ui}p_{u,k})=q_{i, k}+\eta e_{ui}p_{u, k}\\
   \frac{\partial}{\partial b_{i}} S S E=-e_{u i}+\lambda b_{i}\\
   \frac{\partial}{\partial b_{u}} S S E=-e_{u i} +\lambda b_{u}
   $$ 其中，$\eta$ 表示学习率，用于控制步长。当**参数很多**的时候，就是两个矩阵很大的时候，往往容易陷入**过拟合**的困境，需要在目标函数上面加上正则化的损失。

  - 原理图，==分解出的k维特征是模型待学习的参数==
<div align=center>
<img src="https://img-blog.csdnimg.cn/20200823101513233.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3d1emhvbmdxaWFuZw==,size_1,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述" style="zoom:70%;" />
</div>

## FM模型（分解机）
**主要是思想**
+ 优化函数
$$
y = w_0+\sum_{i=1}^nw_ix_i+\sum_{i=1}^{n}\sum_{i+1}^n\lt v_i,v_j\gt x_ix_j
$$ 改进思想就是**为每个$x_i$计算一个embedding，然后将两个向量之间的embedding做内积得到之前所谓的$w_{ij}$**.这使得即使两个特征之前从未在训练集中**同时**出现，只需要$x_i$和其他的$x_k$同时出现过就可以计算出$x_i$的embedding，大大提升了模型的泛化能力。**当交叉项参数$w_{ij}$全为0的时候，整个模型就退化为普通的LR模型**。

上面的公式中： 

- $\omega_{0}$为**全局偏置**；
- $\omega_{i}$是模型第$i$个变量的权重;
- $\omega_{ij} = < v_{i}, v_{j}>$特征$i$和$j$的交叉权重;
- $v_{i} $是第$i$维特征的隐向量;
- $<\cdot, \cdot>$代表向量点积;
- $k(k<<n)$为隐向量的长度，包含 $k$ 个描述特征的因子。

**降低时间复杂度的证明**：
$\sum$运算是线性的，可以互换位置。
$$
\begin{align} \sum_{i=1}^{n-1}{\sum_{j=i+1}^{n}{<v_i,v_j>x_ix_j}}
&= \frac{1}{2}\sum_{i=1}^{n}{\sum_{j=1}^{n}{<v_i,v_j>x_ix_j}} - \frac{1}{2} {\sum_{i=1}^{n}{<v_i,v_i>x_ix_i}} \\
&= \frac{1}{2} \left( \sum_{i=1}^{n}{\sum_{j=1}^{n}{\sum_{f=1}^{k}{v_{i,f}v_{j,f}x_ix_j}}} - \sum_{i=1}^{n}{\sum_{f=1}^{k}{v_{i,f}v_{i,f}x_ix_i}} \right) \\
&= \frac{1}{2}\sum_{f=1}^{k}{\left[ \left( \sum_{i=1}^{n}{v_{i,f}x_i} \right) \cdot \left( \sum_{j=1}^{n}{v_{j,f}x_j} \right) - \sum_{i=1}^{n}{v_{i,f}^2 x_i^2} \right]} \\
&= \frac{1}{2}\sum_{f=1}^{k}{\left[ \left( \sum_{i=1}^{n}{v_{i,f}x_i} \right)^2 - \sum_{i=1}^{n}{v_{i,f}^2 x_i^2} \right]} \end{align}
$$
**解释**：

- $v_{i,f}$ 是一个具体的值；
- 第1个等号：对称矩阵 $W$ 对角线上半部分；
- 第2个等号：把向量内积 $v_{i}$,$v_{j}$ 展开成累加和的形式；
- 第3个等号：提出公共部分；
- 第4个等号： $i$ 和 $j$ 相当于是一样的，表示成平方过程。

## LGBT + LR
+ 思想
  利用GBDT自动进行特征筛选和组合，进而生成新的离散特征向量，再把该特征向量当做LR模型的输入，来产生最后的预测结果，该模型能够综合利用用户、物品和上下文等多种不同的特征，生成较为全面的推荐结果。
+ GBDT模型
  GBDT是通过采用加法模型(即基函数的线性组合），以及不断减小训练过程产生的误差来达到将数据分类或者回归的算法， 其训练过程如下：

  <div align=center>
  <img src="https://img-blog.csdnimg.cn/20200908202508786.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3d1emhvbmdxaWFuZw==,size_1,color_FFFFFF,t_70#pic_center" style="zoom:65%;" />    
  </div>
  gbdt通过多轮迭代，每轮迭代会产生一个弱分类器，每个分类器**在上一轮分类器的残差基础上进行训练**。 gbdt对弱分类器的要求一般是足够简单，并且低方差高偏差。因为训练的过程是通过降低偏差来不断提高最终分类器的精度。由于上述高偏差和简单的要求，每个分类回归树的深度不会很深。最终的总分类器是将每轮训练得到的弱分类器加权求和得到的（也就是加法模型）。  
  GBDT 来解决二分类问题和解决回归问题的本质是一样的，都是通过不断构建决策树的方式，使预测结果一步步的接近目标值，但是二分类问题和回归问题的损失函数是不同的，回归问题中一般使用的是平方损失，而二分类问题中，GBDT和逻辑回归一样，使用的下面这个：

  $$
  L=\arg \min \left[\sum_{i}^{n}-\left(y_{i} \log \left(p_{i}\right)+\left(1-y_{i}\right) \log \left(1-p_{i}\right)\right)\right]
  $$
  其中， $y_i$是第$i$个样本的观测值， 取值要么是0要么是1， 而$p_i$是第$i$个样本的预测值， 取值是0-1之间的概率，由于知道GBDT拟合的残差是当前模型的负梯度， 那么需要求出这个模型的导数，即$\frac{dL}{dp_i}$，对于某个特定的样本，求导的话就可以只考虑它本身，去掉加和号，那么就变成了$\frac{dl}{dp_i}$，其中$l$如下：
  $$
  \begin{aligned}
  l &=-y_{i} \log \left(p_{i}\right)-\left(1-y_{i}\right) \log \left(1-p_{i}\right) \\
  &=-y_{i} \log \left(p_{i}\right)-\log \left(1-p_{i}\right)+y_{i} \log \left(1-p_{i}\right) \\
  &=-y_{i}\left(\log \left(\frac{p_{i}}{1-p_{i}}\right)\right)-\log \left(1-p_{i}\right)
  \end{aligned}
  $$
  根据逻辑回归，$\left(\log \left(\frac{p_{i}}{1-p_{i}}\right)\right)$就是对几率比取了个对数，并且在逻辑回归里面这个式子等于$\theta X$,所以才推出了$p_i=\frac{1}{1+e^-{\theta X}}$的那个形式。这里令$\eta_i=\frac{p_i}{1-p_i}$, 即$p_i=\frac{\eta_i}{1+\eta_i}$, 则上面这个式子变成了：
  $$
  \begin{aligned}
  l &=-y_{i} \log \left(\eta_{i}\right)-\log \left(1-\frac{e^{\log \left(\eta_{i}\right)}}{1+e^{\log \left(\eta_{i}\right)}}\right) \\
  &=-y_{i} \log \left(\eta_{i}\right)-\log \left(\frac{1}{1+e^{\log \left(\eta_{i}\right)}}\right) \\
  &=-y_{i} \log \left(\eta_{i}\right)+\log \left(1+e^{\log \left(\eta_{i}\right)}\right)
  \end{aligned}
  $$
 对$log(\eta_i)$求导， 得

  $$
  \frac{d l}{d \log (\eta_i)}=-y_{i}+\frac{e^{\log \left(\eta_{i}\right)}}{1+e^{\log \left(\eta_{i}\right)}}=-y_i+p_i
  $$

  这样， 就得到了某个训练样本在当前模型的梯度值，那么残差就是$y_i-p_i$。GBDT二分类的这个思想，其实和逻辑回归的思想一样，**逻辑回归是用一个线性模型去拟合$P(y=1|x)$这个事件的对数几率$log\frac{p}{1-p}=\theta^Tx$**， GBDT二分类也是如此，用一系列的梯度提升树去拟合这个对数几率，其分类模型可以表达为：

  $$
  P(Y=1 \mid x)=\frac{1}{1+e^{-F_{M}(x)}}
  $$

  下面具体来看GBDT的生成过程， 构建分类GBDT的步骤有两个：
    1. 初始化GBDT
    和回归问题一样，分类 GBDT 的初始状态也只有一个叶子节点，该节点为所有样本的初始预测值，如下：
$$
F_{0}(x)=\arg \min _{\gamma} \sum_{i=1}^{n} L(y, \gamma)
$$

  	上式 $F$代表GBDT模型，$F_0$是模型的初识状态，该式子的意思是找到一个$\gamma$，使所有样本的 Loss 最小，在这里及下文中，$\gamma$都表示节点的输出，即叶子节点，且它是一个 $log(\eta_i)$ 形式的值(回归值)，在初始状态，$\gamma =F_0$。
  示例：
    <div align=center>
    <img src="https://img-blog.csdnimg.cn/20200910095539432.png#pic_center" alt="在这里插入图片描述" style="zoom:80%;" /> 
    </div>

 	希望构建 GBDT 分类树，它能通过「喜欢爆米花」、「年龄」和「颜色偏好」这 3 个特征来预测某一个样本是否喜欢看电影。把数据代入上面的公式中求Loss:
    $$
    \operatorname{Loss}=L(1, \gamma)+L(1, \gamma)+L(0, \gamma)
    $$
  		为了令其最小，求导让导数为0，则：
    $$
    \operatorname{Loss}=p-1 + p-1+p=0
    $$
 		 于是， 就得到了初始值$p=\frac{2}{3}=0.67, \gamma=log(\frac{p}{1-p})=0.69$, 模型的初识状态$F_0(x)=0.69$

    2. 循环生成决策树
    这里回忆一下回归树的生成步骤，其实有4小步，第一就是计算负梯度值得到残差，第二步是用回归树拟合残差，第三步是计算叶子节点的输出值，第四步是更新模型。 
        1. 计算负梯度得到残差
         $$
         r_{i m}=-\left[\frac{\partial L\left(y_{i}, F\left(x_{i}\right)\right)}{\partial F\left(x_{i}\right)}\right]_{F(x)=F_{m-1}(x)}
         $$
         此处使用$m-1$棵树的模型， 计算每个样本的残差$r_{im}$, 就是上面的$y_i-pi$, 于是例子中， 每个样本的残差：
         <div align=center>
         <img src="https://img-blog.csdnimg.cn/20200910101154282.png#pic_center" alt="在这里插入图片描述" style="zoom:80%;" />
         </div>
    
         2. 使用回归树来拟合$r_{im}$， 这里的$i$表示样本，简单的说就是遍历每个特征，每个特征下遍历每个取值，计算分裂后两组数据的平方损失，找到最小的那个划分节点。 假如我们产生的第2棵决策树如下：
    
         <div align=center>
         <img src="https://img-blog.csdnimg.cn/20200910101558282.png#pic_center" alt="在这里插入图片描述" style="zoom:80%;" />
         </div>
    
         3. 对于每个叶子节点$j$, 计算最佳残差拟合值
         $$
         \gamma_{j m}=\arg \min _{\gamma} \sum_{x \in R_{i j}} L\left(y_{i}, F_{m-1}\left(x_{i}\right)+\gamma\right)
         $$
         意思是，在刚构建的树$m$中，找到每个节点$j$的输出$\gamma_{jm}$, 能使得该节点的loss最小。看一下这个$\gamma$的求解方式.首先，把损失函数写出来，对于左边的第一个样本，有
         $$
         L\left(y_{1}, F_{m-1}\left(x_{1}\right)+\gamma\right)=-y_{1}\left(F_{m-1}\left(x_{1}\right)+\gamma\right)+\log \left(1+e^{F_{m-1}\left(x_{1}\right)+\gamma}\right)
         $$
         这个式子就是上面推导的$l$，因为要用回归树做分类，所以这里把分类的预测概率转换成了对数几率回归的形式，即$log(\eta_i)$，这个就是模型的回归输出值。而如果求这个损失的最小值要，求导解出令损失最小的$\gamma$。但是上面这个式子求导会很麻烦，所以这里介绍了一个技巧就是**使用二阶泰勒公式来近似表示该式，再求导**
         $$
         f(x+\Delta x) \approx f(x)+\Delta x f^{\prime}(x)+\frac{1}{2} \Delta x^{2} f^{\prime \prime}(x)+O(\Delta x)
         $$
         这里就相当于把$L(y_1, F_{m-1}(x_1))$当做常量$f(x)$， $\gamma$作为变量$\Delta x$， 将$f(x)$二阶展开：
         $$
         L\left(y_{1}, F_{m-1}\left(x_{1}\right)+\gamma\right) \approx L\left(y_{1}, F_{m-1}\left(x_{1}\right)\right)+L^{\prime}\left(y_{1}, F_{m-1}\left(x_{1}\right)\right) \gamma+\frac{1}{2} L^{\prime \prime}\left(y_{1}, F_{m-1}\left(x_{1}\right)\right) \gamma^{2}
         $$
         这时候再求导就简单了
         $$
         \frac{d L}{d \gamma}=L^{\prime}\left(y_{1}, F_{m-1}\left(x_{1}\right)\right)+L^{\prime \prime}\left(y_{1}, F_{m-1}\left(x_{1}\right)\right) \gamma
         $$
         Loss最小的时候， 上面的式子等于0， 就可以得到$\gamma$:
         $$
         \gamma_{11}=\frac{-L^{\prime}\left(y_{1}, F_{m-1}\left(x_{1}\right)\right)}{L^{\prime \prime}\left(y_{1}, F_{m-1}\left(x_{1}\right)\right)}
         $$
         **因为分子就是残差(上述已经求到了)， 分母可以通过对残差求导，得到原损失函数的二阶导：**
         $$
         \begin{aligned}
         L^{\prime \prime}\left(y_{1}, F(x)\right) &=\frac{d L^{\prime}}{d \log (\eta_1)} \\
         &=\frac{d}{d \log (\eta_1)}\left[-y_{i}+\frac{e^{\log (\eta_1)}}{1+e^{\log (\eta_1)}}\right] \\
         &=\frac{d}{d \log (\eta_1)}\left[e^{\log (\eta_1)}\left(1+e^{\log (\eta_1)}\right)^{-1}\right] \\
         &=e^{\log (\eta_1)}\left(1+e^{\log (\eta_1)}\right)^{-1}-e^{2 \log (\eta_1)}\left(1+e^{\log (\eta_1)}\right)^{-2} \\
         &=\frac{e^{\log (\eta_1)}}{\left(1+e^{\log (\eta_1)}\right)^{2}} \\
         &=\frac{\eta_1}{(1+\eta_1)}\frac{1}{(1+\eta_1)} \\
         &=p_1(1-p_1)
         \end{aligned}
         $$
         这时候， 就可以算出该节点的输出：
         $$
         \gamma_{11}=\frac{r_{11}}{p_{10}\left(1-p_{10}\right)}=\frac{0.33}{0.67 \times 0.33}=1.49
         $$
         这里的下面$\gamma_{jm}$表示第$m$棵树的第$j$个叶子节点。 接下来是右边节点的输出， 包含样本2和样本3， 同样使用二阶泰勒公式展开：
         $$
         \begin{array}{l}
         L\left(y_{2}, F_{m-1}\left(x_{2}\right)+\gamma\right)+L\left(y_{3}, F_{m-1}\left(x_{3}\right)+\gamma\right) \\
         \approx L\left(y_{2}, F_{m-1}\left(x_{2}\right)\right)+L^{\prime}\left(y_{2}, F_{m-1}\left(x_{2}\right)\right) \gamma+\frac{1}{2} L^{\prime \prime}\left(y_{2}, F_{m-1}\left(x_{2}\right)\right) \gamma^{2} \\
         +L\left(y_{3}, F_{m-1}\left(x_{3}\right)\right)+L^{\prime}\left(y_{3}, F_{m-1}\left(x_{3}\right)\right) \gamma+\frac{1}{2} L^{\prime \prime}\left(y_{3}, F_{m-1}\left(x_{3}\right)\right) \gamma^{2}
         \end{array}
         $$
         求导， 令其结果为0，就会得到， 第1棵树的第2个叶子节点的输出：
         $$
         \begin{aligned}
         \gamma_{21} &=\frac{-L^{\prime}\left(y_{2}, F_{m-1}\left(x_{2}\right)\right)-L^{\prime}\left(y_{3}, F_{m-1}\left(x_{3}\right)\right)}{L^{\prime \prime}\left(y_{2}, F_{m-1}\left(x_{2}\right)\right)+L^{\prime \prime}\left(y_{3}, F_{m-1}\left(x_{3}\right)\right)} \\
         &=\frac{r_{21}+r_{31}}{p_{20}\left(1-p_{20}\right)+p_{30}\left(1-p_{30}\right)} \\
         &=\frac{0.33-0.67}{0.67 \times 0.33+0.67 \times 0.33} \\
         &=-0.77
         \end{aligned}
         $$
         可以看出， 对于任意叶子节点， 我们可以直接计算其输出值：
         $$
         \gamma_{j m}=\frac{\sum_{i=1}^{R_{i j}} r_{i m}}{\sum_{i=1}^{R_{i j}} p_{i, m-1}\left(1-p_{i, m-1}\right)}
         $$
    
          4. 更新模型$F_m(x)$
         $$
         F_{m}(x)=F_{m-1}(x)+\nu \sum_{j=1}^{J_{m}} \gamma_{m}
         $$
    
    这样， 通过多次循环迭代， 就可以得到一个比较强的学习器$F_m(x)$
    
+ GDBT + LR
  + 基本结构
  <div align=center>
  <img src="https://img-blog.csdnimg.cn/20200910161923481.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3d1emhvbmdxaWFuZw==,size_1,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述" style="zoom:67%;" />    
  </div>
  训练时，GBDT 建树的过程相当于自动进行的特征组合和离散化，然后从根结点到叶子节点的这条路径就可以看成是不同特征进行的特征组合，用叶子节点可以唯一的表示这条路径，并作为一个离散特征传入 LR 进行**二次训练**。

+ 注意点
  1. **通过GBDT进行特征组合之后得到的==离散向量和训练数据的原特征一块==作为逻辑回归的输入，而不仅仅全是这种离散特征**
  2. 建树的时候用ensemble建树的原因就是一棵树的表达能力很弱，不足以表达多个有区分性的特征组合，多棵树的表达能力更强一些。GBDT每棵树都在学习前面棵树尚存的不足，迭代多少次就会生成多少棵树。
  3. RF也是多棵树，但从效果上有实践证明不如GBDT。且GBDT前面的树，特征分裂主要体现对多数样本有区分度的特征；后面的树，主要体现的是经过前N颗树，残差仍然较大的少数样本。优先选用在整体上有区分度的特征，再选用针对少数样本有区分度的特征，思路更加合理，这应该也是用GBDT的原因。
  4. 在CRT预估中，GBDT一般会建立两类树(非ID特征建一类， ID类特征建一类)，AD，ID类特征在CTR预估中是非常重要的特征，直接将AD，ID作为feature进行建树不可行，需要为每个AD，ID建GBDT树。
   
# 深度学习方法
==重点是模型结构以及设计思想，并没有在数据上过多深究==

## Neural Collaborative Filtering (NeuralCF)

### 模型结构
<div align="center">
  <img src="https://static001.geekbang.org/resource/image/5f/2c/5ff301f11e686eedbacd69dee184312c.jpg" alt="NeuralCF Model" style="zoom:33%;">
</div>

### 原理
- **输入特征**：用户ID和项目ID转换为OneHot向量，并映射成指定维度的稠密向量，解决冷启动问题。
- **GMF（广义矩阵分解部分）**：用户与物品特征矩阵逐元素点积，通过线性方法学习特征。
- **MLP**：设置用户和物品的MLP，通过特征组合拼接embedding，经过线性层得到MLP的输出。
- **最终输出**：拼接GMF与MLP特征，通过线性层得到最终输出。

### 回归方法：均方误差
$$
L_{sqr}=\sum_{(u,i)\in y\cup y^-}w_{ui}(y_{ui}-\hat{y}_{ui})^2
$$
其中 $w_{ui}$ 为超参数，给每个样本赋权重。

### 似然函数
$$
p(y,y^-|P,Q,\Theta_f)=\prod_{(u,i)\in{y}}\hat{y}_{ui}\prod_{(u,j)\in{y^-}}(1-\hat{y}_{uj})
$$

### 目标函数：对似然函数取负对数
$$
L=-\sum_{(u,i)\in{y}}log\hat{y}_{ui}-\sum_{(u,j)\in{y^-}}log(1-\hat{y}_{uj})=-\sum_{(u,i)\in{y}\cup{y}^-}y_{ui}log \hat{y}_{ui}+(1-y_{ui})log(1-\hat{y}_{ui})
$$
使用随机梯度下降（SGD）进行训练优化。这个函数等价于交叉熵损失函数(binary cross-entropy loss)。通过对NCF进行概率处理，将隐性反馈的推荐问题当做二分类问题来解决。对于负样本 $y^-$，每次迭代均匀地从未观察到交互作用中采样作为负样本，控制采样比例。

**注**：MF是NCF模型的一个特例。

---

## DeepCrossing

**简介**：第一个完整解决特征工程、稀疏向量稠密化、和多层神经网络优化的推荐系统深度学习应用，主要用于点击率预测。

### 模型结构
<div align="center">
  <img src="http://ryluo.oss-cn-chengdu.aliyuncs.com/图片2020100916594542.png" alt="DeepCrossing Model" style="zoom:67%;">
</div>

### 模型原理
- **需解决问题**：
  1. 离散特征编码后过于稀疏，影响神经网络训练。
  2. 特征的自动交叉组合。
  3. 输出层达成优化目标。

- **各层作用**：
  1. **Embedding Layer**：将离散变量转换为OneHot变量，并转换为稠密向量。
  2. **Stacking Layer**：拼接数值型特征和Embedding作为模型输入。
  3. **MLP + Residual Connection**：两层MLP进行特征非线性变换，并与原始输入特征残差连接，最后线性层+激活函数得到预测。

---

## Product Neural Network (PNN)

**工程应用**：通常仅用IPNN（Inner Product Neural Network）。

### 模型结构
<div align="center">
  <img src="http://ryluo.oss-cn-chengdu.aliyuncs.com/图片image-20210308142624189.png" alt="PNN Model" style="zoom:50%;">
</div>

### 原理
- **线性模块**：将所有特征拼接在一起，论文中使用1进行线性变换。
- **非线性模块**：初始化参数矩阵表示特征，使用内积操作得到特征表示。公式如下：

$$
g(f_i,f_j) = <f_i, f_j>
$$

 计算公式：
$$
l_p^n = \sum_{i=1}^N \sum_{j=1}^N (W_p^n)_{i,j} \langle f_i, f_j \rangle
$$
优化后的公式：
$$
l_p = (||\sum_{i=1}^N \theta^1 f_i||^2, ||\sum_{i=1}^N \theta^2 f_i||^2, ..., ||\sum_{i=1}^N \theta^{D_1} f_i||^2)
$$

---

## Wide & Deep

**特性**：Wide部分有助于模型记忆，Deep部分有助于泛化，适合探索新特征组合。
**缺点**： Wide部分的特征组合需要建立在大量工程经验的基础上，极大的依赖人工处理。

### 模型结构
<div align="center">
  <img src="http://ryluo.oss-cn-chengdu.aliyuncs.com/Javaimage-20200910214310877.png" alt="Wide & Deep Model" style="zoom:65%;">
</div>

### 模型原理
- **Wide部分**：广义线性模型，包含原始特征和交叉特征，使用L1正则的FTRL优化，确保Wide部分稀疏化。
- **Deep部分**：DNN模型，包含数值特征和类别特征（需Embedding后输入DNN），通过非线性变换提高模型泛化能力。

**Wide与Deep结合**：通过逻辑回归联合训练Wide和Deep的输出，优化器使用FTRL（Wide部分）和Adagrad（Deep部分）：
$$
P(Y=1|x)=\delta(w_{wide}^T[x,\phi(x)] + w_{deep}^T a^{(lf)} + b)
$$

## DeepFM

### 模型结构
<div align = center>
<img src="https://i-blog.csdnimg.cn/blog_migrate/90a21d85810a74600f340be652d209b2.png#pic_center" style="zoom:50%;" />
</div>

### 模型原理
- **FM层**：这个在推荐系统中的地位非常高，需要重点关注。实际上就是一阶特征和二阶特征拼接之后经过sigmoid激活函数得到logits。
$$
\hat{y}_{FM}(x) = w_0+\sum_{i=1}^N w_ix_i + \sum_{i=1}^N \sum_{j=i+1}^N v_i^T v_j x_ix_j
$$
具体的结构如下：
<div align = center>
<img src="http://ryluo.oss-cn-chengdu.aliyuncs.com/图片image-20210225181340313.png" alt="image-20210225181340313" style="zoom: 67%;" />
</div>
- **Deep**：
结构图：
<div align = center>
<img src="http://ryluo.oss-cn-chengdu.aliyuncs.com/图片image-20210225181010107.png" alt="image-20210225181010107" style="zoom:50%;" />
</div>
用来学习高阶的特征组合，也就是使用MLP对拼接后的稠密特征进行处理。